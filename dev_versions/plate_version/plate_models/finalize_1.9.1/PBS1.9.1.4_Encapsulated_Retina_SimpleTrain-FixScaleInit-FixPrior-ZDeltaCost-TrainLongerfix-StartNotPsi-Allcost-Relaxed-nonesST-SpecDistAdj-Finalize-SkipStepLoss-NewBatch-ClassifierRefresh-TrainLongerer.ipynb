{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7045b43b-a26a-48b0-997a-a393868c300e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# various import statements\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn import cluster\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import inspect\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import softplus, softmax\n",
    "from torch.distributions import constraints\n",
    "import seaborn\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "import pyro.optim\n",
    "from pyro.infer import SVI\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "sc.settings.figdir=os.path.expanduser('~/WbFigures/SpeciesDivergenceNoScaling')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c124fff1-fdcf-48ef-8aa4-a6f7b1177881",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata=sc.read_h5ad(os.path.expanduser('/allen/programs/celltypes/workgroups/rnaseqanalysis/EvoGen/Team/Matthew/data/shekar_retina/retina_mammals.h5ad'))\n",
    "adata.layers['UMIs']=adata.X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bc5015-6807-40c4-8881-147796dd6a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.uns['species_colors']=['blue','red','green','teal','yellow','orange','cyan','pink','purple','magenta','goldenrod','black','grey','beige']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b54a17-7aea-4732-ba98-10bf6ca9140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abaf7fd-e558-4772-9262-ede0e2eab290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/matthew.schmitz/Matthew/code/scANTIPODE/antipode/')\n",
    "import antipode_model\n",
    "from antipode_model import *\n",
    "import model_functions\n",
    "from model_functions import *\n",
    "import model_distributions\n",
    "from model_distributions import *\n",
    "import model_modules\n",
    "from model_modules import *\n",
    "import train_utils\n",
    "from train_utils import *\n",
    "import plotting\n",
    "from plotting import *\n",
    "\n",
    "import importlib\n",
    "antipode_model=importlib.reload(antipode_model)\n",
    "from antipode_model import *\n",
    "\n",
    "model_modules=importlib.reload(model_modules)\n",
    "from model_modules import *\n",
    "\n",
    "model_functions=importlib.reload(model_functions)\n",
    "from model_functions import *\n",
    "\n",
    "model_distributions=importlib.reload(model_distributions)\n",
    "from model_distributions import *\n",
    "\n",
    "train_utils=importlib.reload(train_utils)\n",
    "from train_utils import *\n",
    "\n",
    "plotting=importlib.reload(plotting)\n",
    "from plotting import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1bd92e-d834-4f87-a8e1-3535d47588e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AntipodeTrainingMixin:\n",
    "    '''\n",
    "    Mixin class providing functions to actually run ANTIPODE\n",
    "    can use supervised taxonomy by training only phase2\n",
    "    '''\n",
    "    \n",
    "    def save_params_to_uns(self,prefix=''):\n",
    "        pstore=param_store_to_numpy()\n",
    "        pstore={n:pstore[n] for n in pstore.keys() if not re.search('encoder|classifier|be_nn|\\$\\$\\$',n)}\n",
    "        self.adata_manager.adata.uns[prefix+'param_store']=pstore\n",
    "\n",
    "    def get_antipode_outputs(self,batch_size=2048,device='cuda'):\n",
    "        if 'species_onehot' not in self.adata_manager.adata.obsm.keys():\n",
    "            self.adata_manager.adata.obsm['species_onehot']=numpy_onehot(self.adata_manager.adata.obs['species'].cat.codes)\n",
    "        self.adata_manager.register_new_fields([scvi.data.fields.ObsmField('species_onehot','species_onehot')])\n",
    "    \n",
    "        field_types={\"s\":np.float32,\"species_onehot\":np.float32}\n",
    "        dataloader=scvi.dataloaders.AnnDataLoader(self.adata_manager,batch_size=32,drop_last=False,shuffle=False,data_and_attributes=field_types)#supervised_field_types for supervised step \n",
    "        encoder_outs=batch_output_from_dataloader(dataloader,self.zl_encoder,batch_size=batch_size,device=device)\n",
    "        encoder_outs[0]=self.z_transform(encoder_outs[0])\n",
    "        encoder_out=[x.detach().cpu().numpy() for x in encoder_outs]\n",
    "        classifier_outs=batch_torch_outputs([(self.z_transform(encoder_outs[0]))],self.classifier,batch_size=batch_size,device='cuda')\n",
    "        classifier_out=[x.detach().cpu().numpy() for x in classifier_outs]\n",
    "        return encoder_out,classifier_out\n",
    "\n",
    "    def pretrain_classifier(self,epochs = 5,learning_rate = 0.001,batch_size = 64,prefix='',cluster='kmeans'):\n",
    "        '''basic pytorch training of feed forward classifier to ease step 2'''        \n",
    "        self=self.train()\n",
    "        \n",
    "        model = self.classifier.to(device)\n",
    "        input_tensor =  torch.tensor(self.adata_manager.adata.obsm[prefix+'X_antipode'])  # Your input features tensor, shape [n_samples, n_features]\n",
    "        target_tensor = torch.tensor(self.adata_manager.adata.obsm[cluster+'_onehot'])  # Your target labels tensor, shape [n_samples]    \n",
    "        \n",
    "        # Step 1: Prepare to train\n",
    "        dataset = torch.utils.data.TensorDataset(input_tensor, target_tensor)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        #Training loop\n",
    "        for epoch in range(epochs):\n",
    "            for inputs, targets in dataloader:\n",
    "                # Forward pass\n",
    "                outputs = model(inputs.to(device))\n",
    "                loss = criterion(torch.softmax(outputs[0],-1)[:,-targets.shape[-1]:], targets.to(device))\n",
    "        \n",
    "                # Backward pass and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')     \n",
    "            \n",
    "    def store_outputs(self,device='cuda',prefix=''):\n",
    "        self.save_params_to_uns(prefix='')\n",
    "        self.to('cpu')\n",
    "        self.eval()\n",
    "        antipode_outs=self.get_antipode_outputs(batch_size=2048,device=device)\n",
    "        taxon=antipode_outs[1][0]\n",
    "        self.adata_manager.adata.obsm[prefix+'X_antipode']=antipode_outs[0][0]\n",
    "        self.adata_manager.adata.obs[prefix+'psi']=numpy_centered_sigmoid(antipode_outs[1][1])\n",
    "        level_edges=[numpy_hardmax(self.adata_manager.adata.uns['param_store']['edges_'+str(i)],axis=-1) for i in range(len(self.level_sizes)-1)]\n",
    "        levels=self.tree_convergence_bottom_up.just_propagate(scipy.special.softmax(taxon[...,-self.level_sizes[-1]:],axis=-1),level_edges,s=torch.ones(1))\n",
    "        prop_taxon=np.concatenate(levels,axis=-1)\n",
    "        self.adata_manager.adata.obsm['taxon_probs']=prop_taxon\n",
    "        levels=self.tree_convergence_bottom_up.just_propagate(numpy_hardmax(levels[-1],axis=-1),level_edges,s=torch.ones(1))\n",
    "        for i in range(len(levels)):\n",
    "            cur_clust=prefix+'level_'+str(i)\n",
    "            self.adata_manager.adata.obs[cur_clust]=levels[i].argmax(1)\n",
    "            self.adata_manager.adata.obs[cur_clust]=self.adata_manager.adata.obs[cur_clust].astype(str)\n",
    "        self.adata_manager.adata.obs[prefix+'antipode_cluster'] = self.adata_manager.adata.obs.apply(lambda x: '_'.join([x[prefix+'level_'+str(i)] for i in range(len(levels))]), axis=1)\n",
    "        self.adata_manager.adata.obs[prefix+'antipode_cluster'] = self.adata_manager.adata.obs[prefix+'antipode_cluster'].astype(str)\n",
    "\n",
    "    def fix_scale_factor(self,svi,x,ideal_val=0.1):\n",
    "        o1=svi.evaluate_loss(*x)\n",
    "        s1=self.scale_factor\n",
    "        s2=ideal_val*s1/o1\n",
    "        self.scale_factor=s2\n",
    "\n",
    "    def prepare_phase_2(self,cluster='kmeans',prefix='',epochs = 5):\n",
    "        '''Run this if not running in supervised only mode (JUST phase2 with provided obsm clustering), \n",
    "        runs kmeans if cluster=kmeans, else uses the obs column provided by cluster'''\n",
    "        if cluster=='kmeans':\n",
    "            kmeans = sklearn.cluster.MiniBatchKMeans(n_clusters=self.level_sizes[-1],init='k-means++',max_iter=1000,reassignment_ratio=0.001,n_init=100,random_state=0).fit(self.adata_manager.adata.obsm['X_antipode'])\n",
    "            self.adata_manager.adata.obs['kmeans']=kmeans.labels_\n",
    "            self.adata_manager.adata.obs['kmeans']=self.adata_manager.adata.obs['kmeans'].astype(int).astype('category')\n",
    "            self.adata_manager.adata.obsm['kmeans_onehot']=numpy_onehot(self.adata_manager.adata.obs['kmeans'].cat.codes,num_classes=self.level_sizes[-1])\n",
    "        else:\n",
    "            self.adata_manager.adata.obs[cluster]=self.adata_manager.adata.obs[cluster].astype('category')\n",
    "            self.adata_manager.adata.obsm[cluster+'_onehot']=numpy_onehot(self.adata_manager.adata.obs[cluster].cat.codes,num_classes=self.level_sizes[-1])\n",
    "        \n",
    "        self.adata_manager.register_new_fields([make_field('taxon',('obsm',cluster+'_onehot'))])\n",
    "        if epochs is not None:\n",
    "            self.pretrain_classifier(cluster=cluster,prefix=prefix,epochs=epochs)\n",
    "        kmeans_means=group_aggr_anndata(self.adata_manager.adata,[cluster], agg_func=np.mean,layer='X_antipode',obsm=True)[0]\n",
    "        new_locs=torch.concatenate(\n",
    "            [pyro.param('locs').new_zeros(sum(self.level_sizes[:-1]),pyro.param('locs').shape[1]),\n",
    "             torch.tensor(kmeans_means-kmeans_means.mean(0),device=pyro.param('locs').device).float()],\n",
    "             axis=0).float()\n",
    "        new_locs[0,:]=torch.tensor(kmeans_means.mean(0)).float()\n",
    "        self.adata_manager.adata.obs[cluster].astype(int)\n",
    "        new_scales=group_aggr_anndata(self.adata_manager.adata,[cluster], agg_func=np.std,layer='X_antipode',obsm=True)[0]\n",
    "        new_scales=torch.concatenate(\n",
    "            [0.0001*self.scale_init_val*new_locs.new_ones(sum(self.level_sizes[:-1]),pyro.param('locs').shape[1],requires_grad=True),\n",
    "             torch.tensor(new_scales+1e-10,device=pyro.param('scales').device,requires_grad=True)],axis=0).float()\n",
    "        self.adata_manager.adata.obs[cluster].astype(str)\n",
    "        pyro.get_param_store().__setitem__('locs',new_locs)\n",
    "        pyro.get_param_store().__setitem__('locs_dynam',new_locs.new_zeros(new_locs.shape))\n",
    "        pyro.get_param_store().__setitem__('scales',new_scales)\n",
    "        self.adata_manager.adata.obs[cluster]=self.adata_manager.adata.obs[cluster].astype(str)\n",
    "        pyro.get_param_store().__setitem__('discov_dm',new_locs.new_zeros(pyro.param('discov_dm').shape))\n",
    "        pyro.get_param_store().__setitem__('batch_dm',new_locs.new_zeros(pyro.param('batch_dm').shape))\n",
    "        pyro.get_param_store().__setitem__('discov_di',new_locs.new_zeros(pyro.param('discov_di').shape))\n",
    "        pyro.get_param_store().__setitem__('batch_di',new_locs.new_zeros(pyro.param('batch_di').shape))\n",
    "        pyro.get_param_store().__setitem__('cluster_intercept',new_locs.new_zeros(pyro.param('cluster_intercept').shape))\n",
    "    \n",
    "    def common_training_loop(self, dataloader, max_steps, scheduler, svi, print_every, device, steps=0):\n",
    "        self.losses = []\n",
    "        pbar = tqdm.tqdm(total=max_steps, position=0)\n",
    "        while steps < max_steps:\n",
    "            for x in dataloader:\n",
    "                x['step'] = torch.ones(1).to(device) * steps\n",
    "                x = [x[k].to(device) if k in x.keys() else torch.zeros(1) for k in self.args]\n",
    "                if self.scale_factor == 1.:\n",
    "                    self.fix_scale_factor(svi, x)\n",
    "                pbar.update(1)\n",
    "                loss = svi.step(*x)\n",
    "                steps += 1\n",
    "                if hasattr(scheduler, 'step'):\n",
    "                    scheduler.step()\n",
    "                if steps >= max_steps - 1 :\n",
    "                    break\n",
    "                \n",
    "                self.losses.append(loss)\n",
    "                if steps % print_every == 0:\n",
    "                    pbar.write(f\"[Step {steps:02d}]  Loss: {np.mean(self.losses[-print_every:]):.5f}\")\n",
    "        pbar.close()\n",
    "        try:\n",
    "            self.allDone()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def setup_scheduler(self, max_learning_rate, max_steps, one_cycle_lr):\n",
    "        if one_cycle_lr:\n",
    "            return pyro.optim.OneCycleLR({\n",
    "                'max_lr': max_learning_rate,\n",
    "                'total_steps': max_steps,\n",
    "                'div_factor': 100,\n",
    "                'optim_args': {},\n",
    "                'optimizer': torch.optim.Adam\n",
    "            })\n",
    "        else:\n",
    "            return pyro.optim.ClippedAdam({\n",
    "                'lr': max_learning_rate,\n",
    "                'lrd': (1 - (5e-6))\n",
    "            })\n",
    "\n",
    "    def train_phase(self, phase, max_steps, print_every=10000, device='cuda', max_learning_rate=0.001, num_particles=1, one_cycle_lr=True, steps=0, batch_size=32,freeze_encoder=None):\n",
    "        self.scale_factor=1.\n",
    "        freeze_encoder = True if freeze_encoder is None and phase == 2 else freeze_encoder\n",
    "        freeze_encoder = False if freeze_encoder is None else  freeze_encoder\n",
    "        self.set_freeze_encoder(freeze_encoder) \n",
    "        supervised_field_types=self.field_types.copy()\n",
    "        supervised_fields=self.fields.copy()\n",
    "        supervised_field_types[\"taxon\"]=np.float32\n",
    "        field_types=self.field_types if phase != 2 else supervised_field_types\n",
    "        dataloader = scvi.dataloaders.AnnDataLoader(self.adata_manager, batch_size=batch_size, drop_last=True, shuffle=True, data_and_attributes=field_types)\n",
    "        scheduler = self.setup_scheduler(max_learning_rate, max_steps, one_cycle_lr)\n",
    "        elbo_class = pyro.infer.JitTrace_ELBO\n",
    "        elbo = elbo_class(num_particles=num_particles, strict_enumeration_warning=False)\n",
    "        hide_params=[name for name in pyro.get_param_store() if re.search('encoder',name)]\n",
    "        guide=self.guide if not self.freeze_encoder else poutine.block(self.guide,hide=hide_params)\n",
    "        svi = SafeSVI(self.model, guide, scheduler, elbo,clip_std_multiplier=5.0)  \n",
    "        self.train()\n",
    "        self.zl_encoder.eval() if self.freeze_encoder else self.zl_encoder.train()\n",
    "        self = self.to(device)\n",
    "        self.set_approx(phase == 1)\n",
    "        return self.common_training_loop(dataloader, max_steps, scheduler, svi, print_every, device, steps)\n",
    "        \n",
    "    def allDone(self):\n",
    "        print(\"Finished training!\")\n",
    "        from IPython.display import Audio, display\n",
    "        display(Audio(url='https://notification-sounds.com/soundsfiles/Meditation-bell-sound.mp3', autoplay=True))\n",
    "\n",
    "\n",
    "class ANTIPODE(PyroBaseModuleClass,AntipodeTrainingMixin):#\n",
    "    '''\n",
    "    ANTIPODE (Single Cell Ancestral Node Taxonomy Inference by Parcellation of Differential Expression) \n",
    "    is a variational inference model developed for the simultaneous analysis (DE) and \n",
    "    categorization (taxonomy generation) of cell types across evolution (or now any covariate) using single-cell RNA-seq data.\n",
    "    \n",
    "\n",
    "    Parameters:\n",
    "    adata (AnnData): An AnnData object containing the single-cell dataset.\n",
    "    discov_pair (tuple): A tuple indicating the key and location of the discovery covariate \n",
    "                         in the AnnData object. Format: ('key', 'location'), where location is \n",
    "                         either 'obs' or 'obsm'.\n",
    "    batch_pair (tuple): A tuple indicating the key and location of the batch covariate \n",
    "                        in the AnnData object. Format: ('key', 'location'), where location is \n",
    "                        either 'obs' or 'obsm'.\n",
    "    num_var (int): Number of variables (features) in the dataset.\n",
    "    level_sizes (list of int, optional): Sizes of each level in the model's hierarchical structure. \n",
    "                                         Defaults to [1, 10, 25, 50].\n",
    "    num_latent (int, optional): Number of latent dimensions. Defaults to 50.\n",
    "    scale_factor (float, optional): Scaling factor for data normalization. If None, it is inferred from the data.\n",
    "    prior_scale (float, optional): Scale of the Laplace prior distributions. Defaults to 100.\n",
    "    dcd_prior (float, optional): Scale of the prior for the decoder. If None, defaults to a specific inferred value.\n",
    "    theta_prior (float, optional): Init value for the inverse dispersion of the negative binomial.\n",
    "    bi_depth (int, optional): Depth of the tree for the approximation of batch by identity effects. Defaults to 2.\n",
    "    num_batch_embed (int, optional): Number of batch embeddings. Defaults to 10.\n",
    "    classifier_hidden (list of int, optional): Sizes of hidden layers for the classifier network. Defaults to [3000, 3000, 3000].\n",
    "    encoder_hidden (list of int, optional): Sizes of hidden layers for the encoder network. Defaults to [6000, 5000, 3000, 1000].\n",
    "    '''\n",
    "    def __init__(self, adata, discov_pair, batch_pair, layer, level_sizes=[1,10,100],\n",
    "                 num_latent=50,scale_factor=None, prior_scale=100,dcd_prior=None,\n",
    "                 use_psi=True,loc_as_param=False,zdw_as_param=False,intercept_as_param=False,\n",
    "                 num_batch_embed=10,theta_prior=50.,scale_init_val=0.01,bi_depth=2,dist_normalize=False,\n",
    "                 classifier_hidden=[3000,3000,3000],encoder_hidden=[6000,5000,3000,1000],z_transform=None):\n",
    "\n",
    "        pyro.clear_param_store()\n",
    "\n",
    "        # Determine num_discov and num_batch from the AnnData object\n",
    "        self.discov_loc, self.discov_key = discov_pair\n",
    "        self.batch_loc, self.batch_key = batch_pair\n",
    "        self.num_discov = adata.obsm[self.discov_key].shape[-1] if self.discov_loc == 'obsm' else len(adata.obs[self.discov_key].unique())\n",
    "        self.num_batch = adata.obsm[self.batch_key].shape[-1] if self.batch_loc == 'obsm' else len(adata.obs[self.batch_key].unique())\n",
    "        self.design_matrix = (self.discov_loc == 'obsm')\n",
    "        self.layer=layer\n",
    "\n",
    "        self._setup_adata_manager_store: dict[str, type[scvi.data.AnnDataManager]] = {}\n",
    "        self.num_var = adata.layers[layer].shape[-1]\n",
    "        self.num_latent = num_latent\n",
    "        self.scale_factor = 1.0#scale_factor if scale_factor is not None else 2e2 / (self.num_var * num_particles * num_latent)\n",
    "        self.num_batch_embed = num_batch_embed\n",
    "        self.temperature = 0.1\n",
    "        self.epsilon = 1e-5\n",
    "        self.approx = False\n",
    "        self.prior_scale = prior_scale\n",
    "        self.use_psi=use_psi\n",
    "        self.loc_as_param=loc_as_param\n",
    "        self.zdw_as_param=zdw_as_param\n",
    "        self.intercept_as_param=intercept_as_param\n",
    "        self.theta_prior=theta_prior\n",
    "        self.scale_init_val=scale_init_val\n",
    "        self.level_sizes=level_sizes\n",
    "        self.num_labels=sum(level_sizes)\n",
    "        self.bi_depth = bi_depth\n",
    "        self.bi_depth = sum(self.level_sizes[:self.bi_depth])\n",
    "        self.dist_normalize=dist_normalize\n",
    "\n",
    "        self.dcd_prior=torch.zeros((self.num_discov,self.num_var)) if dcd_prior is None else dcd_prior#Use this for \n",
    "                \n",
    "        # Initialize plates to be used during sampling\n",
    "        self.var_plate=pyro.plate('var_plate',self.num_var,dim=-1)\n",
    "        self.discov_plate=pyro.plate('discov_plate',self.num_discov,dim=-3)\n",
    "        self.batch_plate=pyro.plate('batch_plate',self.num_batch,dim=-3)\n",
    "        self.latent_plate=pyro.plate('latent_plate',self.num_latent,dim=-1)\n",
    "        self.latent_plate2=pyro.plate('latent_plate2',self.num_latent,dim=-2)\n",
    "        self.label_plate=pyro.plate('label_plate',self.num_labels,dim=-2)\n",
    "        self.batch_embed_plate=pyro.plate('batch_embed_plate',self.num_batch_embed,dim=-3)\n",
    "        self.bi_depth_plate=pyro.plate('bi_depth_plate',self.bi_depth,dim=-2)\n",
    "\n",
    "        #Initialize MAP inference modules\n",
    "        self.dm=MAPLaplaceModule(self,'discov_dm',[self.num_discov,self.num_labels,self.num_latent],[self.discov_plate,self.label_plate,self.latent_plate])\n",
    "        self.bm=MAPLaplaceModule(self,'batch_dm',[self.num_batch,self.num_labels,self.num_latent],[self.batch_plate,self.label_plate,self.latent_plate])\n",
    "        self.di=MAPLaplaceModule(self,'discov_di',[self.num_discov,self.num_labels,self.num_var],[self.discov_plate,self.label_plate,self.var_plate])\n",
    "        self.bei=MAPLaplaceModule(self,'batch_di',[self.num_batch_embed,self.bi_depth,self.num_var],[self.batch_embed_plate,self.bi_depth_plate,self.var_plate])\n",
    "        self.ci=MAPLaplaceModule(self,'cluster_intercept',[self.num_labels, self.num_var],[self.label_plate,self.var_plate],param_only=self.intercept_as_param)\n",
    "        self.dc=MAPLaplaceModule(self,'discov_dc',[self.num_discov,self.num_latent,self.num_var],[self.discov_plate,self.latent_plate2,self.var_plate])\n",
    "        self.zdw=MAPLaplaceModule(self,'z_decoder_weight',[self.num_latent,self.num_var],[self.latent_plate2,self.var_plate],init_val=((2/self.num_latent)*(torch.rand(self.num_latent,self.num_var)-0.5)),param_only=self.zdw_as_param)\n",
    "        self.zl=MAPLaplaceModule(self,'locs',[self.num_labels,self.num_latent],[self.label_plate,self.latent_plate],param_only=self.loc_as_param)\n",
    "        self.zs=MAPHalfCauchyModule(self,'scales',[self.num_labels,self.num_latent],[self.label_plate,self.latent_plate],init_val=self.scale_init_val*torch.ones(self.num_labels,self.num_latent),constraint=constraints.positive,param_only=False)\n",
    "        self.zld=MAPLaplaceModule(self,'locs_dynam',[self.num_labels,self.num_latent],[self.label_plate,self.latent_plate],param_only=False)\n",
    "        \n",
    "        self.tree_edges=TreeEdges(self,straight_through=False)\n",
    "        self.tree_convergence=TreeConvergence(self)        \n",
    "        self.tree_convergence_bottom_up=TreeConvergenceBottomUp(self)        \n",
    "        self.z_transform=null_function if z_transform is None else z_transform#centered_sigmoid#torch.special.expit\n",
    "\n",
    "        if self.design_matrix:\n",
    "            fields={'s':('layers',self.layer),\n",
    "            'discov_ind':('obsm',self.discov_key),\n",
    "            'batch_ind':('obsm',self.batch_key)}\n",
    "            field_types={\"s\":np.float32,\"batch_ind\":np.float32,\"discov_ind\":np.float32}\n",
    "        else:\n",
    "            fields={'s':('layers',self.layer),\n",
    "            'discov_ind':('obs',self.discov_key),\n",
    "            'batch_ind':('obs',self.batch_key)}\n",
    "            field_types={\"s\":np.float32,\"batch_ind\":np.int64,\"discov_ind\":np.int64}\n",
    "\n",
    "        self.fields=fields\n",
    "        self.field_types=field_types\n",
    "        self.setup_anndata(adata, {'discov_ind': discov_pair, 'batch_ind': batch_pair}, self.field_types)\n",
    "        \n",
    "        super().__init__()\n",
    "        # Setup the various neural networks used in the model and guide\n",
    "        self.z_decoder=ZDecoder(num_latent=self.num_latent, num_var=self.num_var, hidden_dims=[])        \n",
    "        self.zl_encoder=ZLEncoder(num_var=self.num_var,hidden_dims=encoder_hidden,num_cat_input=self.num_discov,\n",
    "                    outputs=[(self.num_latent,None),(self.num_latent,softplus)])\n",
    "        \n",
    "        self.classifier=Classifier(num_latent=self.num_latent,hidden_dims=classifier_hidden,\n",
    "                    outputs=[(self.num_labels,None),(1,None),(1,softplus)])\n",
    "\n",
    "        #self.bc_nn=SimpleFFNN(in_dim=self.num_batch,hidden_dims=[200,200,50,5],\n",
    "        #            out_dim=self.num_var*self.num_latent)\n",
    "        #Too large to exactly model gene-level batch effects for all cluster x batch\n",
    "        self.be_nn=SimpleFFNN(in_dim=self.num_batch,hidden_dims=[1000,500,500],\n",
    "                    out_dim=self.num_batch_embed)\n",
    "        \n",
    "        self.epsilon = 0.006\n",
    "        #Initialize model in approximation mode\n",
    "        self.approx=False\n",
    "        self.prior_scale=prior_scale\n",
    "        self.args=inspect.getfullargspec(self.model).args[1:]#skip self\n",
    "\n",
    "    def setup_anndata(self,adata: anndata.AnnData,fields,field_types,**kwargs,):\n",
    "        \n",
    "        anndata_fields=[make_field(x,self.fields[x]) for x in self.fields.keys()]\n",
    "        \n",
    "        adata_manager = scvi.data.AnnDataManager(\n",
    "            fields=anndata_fields\n",
    "        )\n",
    "        adata_manager.register_fields(adata, **kwargs)\n",
    "        self.register_manager(adata_manager)\n",
    "        if fields['discov_ind'][0]=='obsm':\n",
    "            self.design_matrix=True\n",
    "            if fields['batch_ind'][0]!='obsm':\n",
    "                raise Exception(\"If discov is design matrix, batch must be as well!\")\n",
    "\n",
    "    def register_manager(self, adata_manager: scvi.data.AnnDataManager):\n",
    "        adata_id = adata_manager.adata_uuid\n",
    "        self._setup_adata_manager_store[adata_id] = adata_manager\n",
    "        self.adata_manager=adata_manager\n",
    "    \n",
    "    def set_approx(self,b: bool):\n",
    "        self.approx=b\n",
    "\n",
    "    def set_freeze_encoder(self,b: bool):\n",
    "        self.freeze_encoder=b\n",
    "        \n",
    "    def model(self, s,discov_ind=torch.zeros(1),batch_ind=torch.zeros(1),step=torch.ones(1),taxon=torch.zeros(1)):\n",
    "        # Register various nn.Modules (i.e. the decoder/encoder networks) with Pyro\n",
    "        pyro.module(\"antipode\", self)\n",
    "\n",
    "        if not self.design_matrix:\n",
    "            batch=index_to_onehot(batch_ind,[s.shape[0],self.num_batch]).to(s.device)\n",
    "            discov=index_to_onehot(discov_ind,[s.shape[0],self.num_discov]).to(s.device)\n",
    "            batch_ind=batch_ind.squeeze()\n",
    "            discov_ind=discov_ind.squeeze()\n",
    "        else:\n",
    "            batch=batch_ind\n",
    "            discov=discov_ind\n",
    "        \n",
    "        minibatch_plate=pyro.plate(\"minibatch_plate\", s.shape[0],dim=-1)\n",
    "        minibatch_plate2=pyro.plate(\"minibatch_plate2\", s.shape[0],dim=-2)\n",
    "        l = s.sum(1).unsqueeze(-1)\n",
    "        \n",
    "        # Scale all sample statements for numerical stability\n",
    "        with poutine.scale(scale=self.scale_factor):\n",
    "            # Counts parameter of NB (variance of the observation distribution)\n",
    "            s_theta = pyro.param(\"s_inverse_dispersion\", self.theta_prior * s.new_ones(self.num_var),\n",
    "                               constraint=constraints.positive)\n",
    "            #Weak overall histogram normalization\n",
    "            discov_mul = pyro.param(\"discov_mul\", s.new_ones(self.num_discov,1),constraint=constraints.positive) if self.dist_normalize else s.new_ones(self.num_discov)\n",
    "            discov_int = pyro.param(\"discov_int\", s.new_zeros(self.num_discov,1)) if self.dist_normalize else s.new_zeros(self.num_discov)\n",
    "\n",
    "            dcd=pyro.param(\"discov_constitutive_de\", self.dcd_prior.to(s.device))\n",
    "            level_edges=self.tree_edges.model_sample(s,approx=self.approx)\n",
    "            \n",
    "            with minibatch_plate:\n",
    "                batch_embed=centered_sigmoid(pyro.sample('batch_embed', dist.Laplace(s.new_zeros(self.num_batch_embed),\n",
    "                                self.prior_scale*s.new_ones(self.num_batch_embed),validate_args=True).to_event(1)))\n",
    "                beta_prior_a=1.*s.new_ones(self.num_labels)\n",
    "                beta_prior_a[0]=10. #0 block is consititutive\n",
    "                if self.approx:#Bernoulli blocks approx?\n",
    "                    taxon_probs = pyro.sample(\"taxon_probs\", dist.Beta(beta_prior_a,s.new_ones(self.num_labels),validate_args=True).to_event(1))\n",
    "                    taxon = pyro.sample('taxon',dist.RelaxedBernoulli(temperature=0.1*s.new_ones(1),probs=taxon_probs).to_event(1))\n",
    "                else:\n",
    "                    taxon_probs=pyro.sample('taxon_probs',dist.Dirichlet(s.new_ones(s.shape[0],self.level_sizes[-1]),validate_args=True))\n",
    "                    if sum(taxon.shape) > 1:#Supervised?\n",
    "                        if taxon.shape[-1]==self.num_labels:#Totally supervised?\n",
    "                            pass\n",
    "                        else:#Only bottom layer is supervised?\n",
    "                            taxon = pyro.sample(\"taxon\", dist.OneHotCategorical(probs=taxon_probs,validate_args=True),obs=taxon)\n",
    "                            taxon = self.tree_convergence_bottom_up.just_propagate(taxon,level_edges,s) if self.freeze_encoder else self.tree_convergence_bottom_up.just_propagate(taxon,level_edges,s)\n",
    "                    else:#Unsupervised\n",
    "                        taxon = pyro.sample(\"taxon\", \n",
    "                                         model_distributions.SafeAndRelaxedOneHotCategorical(temperature=self.temperature*s.new_ones(1),probs=taxon_probs,validate_args=True))                    \n",
    "                        taxon = self.tree_convergence_bottom_up.just_propagate(taxon,level_edges,s) if self.freeze_encoder else self.tree_convergence_bottom_up.just_propagate(taxon,level_edges,s)\n",
    "                    taxon = torch.concat(taxon,-1)\n",
    "                    taxon_probs=self.tree_convergence_bottom_up.just_propagate(taxon_probs[...,-self.level_sizes[-1]:],level_edges,s) if self.freeze_encoder else self.tree_convergence_bottom_up.just_propagate(taxon_probs[...,-self.level_sizes[-1]:],level_edges,s)\n",
    "                    taxon_probs=torch.cat(taxon_probs,-1)\n",
    "                   \n",
    "            locs=self.zl.model_sample(s,scale=fest([taxon_probs],-1))\n",
    "            scales=self.zs.model_sample(s,scale=fest([taxon_probs],-1))\n",
    "            locs_dynam=self.zld.model_sample(s,scale=fest([taxon_probs],-1))\n",
    "            discov_dm=self.dm.model_sample(s,scale=fest([discov,taxon_probs],-1))\n",
    "            discov_di=self.di.model_sample(s,scale=fest([discov,taxon_probs],-1))\n",
    "            batch_dm=self.bm.model_sample(s,scale=fest([batch,taxon_probs],-1))\n",
    "            \n",
    "            bei=self.bei.model_sample(s,scale=fest([batch_embed.abs(),taxon_probs[...,:self.bi_depth]],-1))\n",
    "            cluster_intercept=self.ci.model_sample(s,scale=fest([taxon_probs],-1))\n",
    "            \n",
    "            with minibatch_plate:\n",
    "                bi=torch.einsum('...bi,...ijk->...bjk',batch_embed,bei)\n",
    "                bi=torch.einsum('...bj,...bjk->...bk',taxon[...,:self.bi_depth],bi)\n",
    "                psi = centered_sigmoid(pyro.sample('psi',dist.Laplace(s.new_zeros(s.shape[0],1),self.prior_scale*s.new_ones(s.shape[0],1)).to_event(1)))\n",
    "                psi = 0 if not self.use_psi or self.approx else psi\n",
    "                this_locs=oh_index(locs,taxon)\n",
    "                this_scales=oh_index(scales,taxon)\n",
    "                z=pyro.sample('z', dist.Normal(this_locs,this_scales+self.epsilon,validate_args=True).to_event(1))\n",
    "                pyro.sample('z_loc', dist.Laplace(this_locs,0.5*self.prior_scale*s.new_ones(s.shape[0],self.num_latent),validate_args=True).to_event(1))\n",
    "\n",
    "            cur_discov_dm = oh_index1(discov_dm, discov_ind) if self.design_matrix else discov_dm[discov_ind]\n",
    "            cur_batch_dm = oh_index1(batch_dm, batch_ind) if self.design_matrix else batch_dm[batch_ind]\n",
    "            cur_dcd = oh_index(dcd, discov) if self.design_matrix else  dcd[discov_ind]\n",
    "            \n",
    "            z=z+oh_index2(cur_discov_dm,taxon) + oh_index2(cur_batch_dm,taxon)+(oh_index(locs_dynam,taxon)*psi)\n",
    "            z=self.z_transform(z)                \n",
    "            pseudo_z=oh_index(locs,taxon_probs)+oh_index2(discov_dm[discov_ind],taxon_probs) + oh_index2(batch_dm[batch_ind],taxon_probs)+(oh_index(locs_dynam,taxon_probs)*psi)\n",
    "            pseudo_z=self.z_transform(pseudo_z)\n",
    "            z_decoder_weight=self.zdw.model_sample(s,scale=fest([pseudo_z.abs()],-1))\n",
    "            discov_dc=self.dc.model_sample(s,scale=fest([discov,pseudo_z.abs()],-1))\n",
    "            cur_discov_di = oh_index1(discov_di, discov_ind) if self.design_matrix else discov_di[discov_ind]\n",
    "            cur_discov_dc = oh_index1(discov_dc, discov_ind) if self.design_matrix else discov_dc[discov_ind]\n",
    "            cur_discov_di=oh_index2(cur_discov_di,taxon)\n",
    "            cur_cluster_intercept=oh_index(cluster_intercept,taxon)\n",
    "            \n",
    "            mu=torch.einsum('...bi,...bij->...bj',z,z_decoder_weight+cur_discov_dc)#+bc\n",
    "            spliced_mu=mu+cur_dcd+cur_discov_di+cur_cluster_intercept+bi\n",
    "            norm_spliced_mu=spliced_mu*discov_mul[discov_ind]+discov_int[discov_ind]\n",
    "            spliced_out=torch.softmax(norm_spliced_mu,dim=-1)\n",
    "            log_mu = (l * spliced_out + 1e-6).log()\n",
    "            \n",
    "            with self.var_plate,minibatch_plate2:\n",
    "                s_dist = dist.NegativeBinomial(total_count=s_theta,logits=log_mu-s_theta.log(),validate_args=True)\n",
    "                s_out=pyro.sample(\"s\", s_dist, obs=s.int())\n",
    "\n",
    "    \n",
    "    # The guide specifies the variational distribution\n",
    "    def guide(self, s,discov_ind=torch.zeros(1),batch_ind=torch.zeros(1),step=torch.ones(1),taxon=torch.zeros(1)):\n",
    "        pyro.module(\"antipode\", self)\n",
    "        \n",
    "        if not self.design_matrix:\n",
    "            batch=index_to_onehot(batch_ind,[s.shape[0],self.num_batch]).to(s.device)\n",
    "            discov=index_to_onehot(discov_ind,[s.shape[0],self.num_discov]).to(s.device)\n",
    "            batch_ind=batch_ind.squeeze()\n",
    "            discov_ind=discov_ind.squeeze()\n",
    "        else:\n",
    "            batch=batch_ind\n",
    "            discov=discov_ind\n",
    "        \n",
    "        minibatch_plate=pyro.plate(\"minibatch_plate\", s.shape[0])\n",
    "        \n",
    "        with poutine.scale(scale=self.scale_factor):\n",
    "            level_edges=self.tree_edges.guide_sample(s,approx=self.approx) \n",
    "            with minibatch_plate:\n",
    "                batch_embed=self.be_nn(batch)\n",
    "                batch_embed=pyro.sample('batch_embed', dist.Delta(batch_embed,validate_args=True).to_event(1))\n",
    "                if self.freeze_encoder:#Bernoulli particles approx?\n",
    "                    with torch.no_grad():\n",
    "                        print('nograd')\n",
    "                        z_loc, z_scale= self.zl_encoder(s,discov)\n",
    "                        z_loc=z_loc.detach()\n",
    "                        z_scale=z_scale.detach()\n",
    "                else:\n",
    "                    z_loc, z_scale= self.zl_encoder(s,discov)\n",
    "                z=pyro.sample('z',dist.Normal(z_loc,z_scale+self.epsilon).to_event(1))\n",
    "                pyro.sample('z_loc',dist.Delta(z_loc).to_event(1))\n",
    "                z=self.z_transform(z)\n",
    "                taxon_logits,psi_loc,psi_scale=self.classifier(z)\n",
    "                psi=centered_sigmoid(pyro.sample('psi',dist.Normal(psi_loc,psi_scale+self.epsilon).to_event(1)))\n",
    "                psi = 0 if not self.use_psi or self.approx else psi\n",
    "                if self.approx:\n",
    "                    taxon_dist = dist.Delta(safe_sigmoid(taxon_logits),validate_args=True).to_event(1)\n",
    "                    taxon_probs = pyro.sample(\"taxon_probs\", taxon_dist)\n",
    "                    taxon = pyro.sample('taxon',dist.RelaxedBernoulli(temperature=self.temperature*s.new_ones(1),probs=taxon_probs).to_event(1))\n",
    "                else:\n",
    "                    taxon_probs=pyro.sample('taxon_probs',dist.Delta(safe_softmax(taxon_logits[...,-self.level_sizes[-1]:],eps=1e-5)).to_event(1))\n",
    "                    if sum(taxon.shape) > 1:\n",
    "                        pass\n",
    "                    else:\n",
    "                        taxon = pyro.sample(\"taxon\", \n",
    "                                         model_distributions.SafeAndRelaxedOneHotCategorical(temperature=self.temperature*s.new_ones(1),probs=taxon_probs,validate_args=True))                    \n",
    "                    if taxon.shape[-1]<self.num_labels:\n",
    "                        taxon = self.tree_convergence_bottom_up.just_propagate(taxon,level_edges,s) if self.freeze_encoder else self.tree_convergence_bottom_up.just_propagate(taxon,level_edges,s)\n",
    "                        taxon = torch.concat(taxon,-1)\n",
    "                    taxon_probs=self.tree_convergence_bottom_up.just_propagate(taxon_probs[...,-self.level_sizes[-1]:],level_edges,s) if self.freeze_encoder else self.tree_convergence_bottom_up.just_propagate(taxon_probs[...,-self.level_sizes[-1]:],level_edges,s)\n",
    "                    taxon_probs=torch.cat(taxon_probs,-1)\n",
    "\n",
    "            locs=self.zl.guide_sample(s,scale=fest([taxon_probs],-1))\n",
    "            scales=self.zs.guide_sample(s,scale=fest([taxon_probs],-1))\n",
    "            locs_dynam=self.zld.guide_sample(s,scale=fest([taxon_probs],-1))\n",
    "            discov_dm=self.dm.guide_sample(s,scale=fest([discov,taxon_probs],-1))\n",
    "            batch_dm=self.bm.guide_sample(s,scale=fest([batch,taxon_probs],-1))\n",
    "            discov_di=self.di.guide_sample(s,scale=fest([discov,taxon_probs],-1))\n",
    "            cluster_intercept=self.ci.guide_sample(s,scale=fest([taxon_probs],-1))\n",
    "            bei=self.bei.guide_sample(s,scale=fest([batch_embed.abs(),taxon_probs[...,:self.bi_depth]],-1))#maybe should be abs sum bei\n",
    "            \n",
    "            if self.design_matrix:\n",
    "                z=z+oh_index2(oh_index1(discov_dm,discov_ind),taxon) + oh_index2(oh_index1(batch_dm,batch_ind),taxon)+(oh_index(locs_dynam,taxon)*psi)\n",
    "            else:\n",
    "                z=z+oh_index2(discov_dm[discov_ind],taxon) + oh_index2(batch_dm[batch_ind],taxon)+(oh_index(locs_dynam,taxon)*psi)\n",
    "            z=self.z_transform(z)\n",
    "            pseudo_z=oh_index(locs,taxon_probs)+oh_index2(discov_dm[discov_ind],taxon_probs) + oh_index2(batch_dm[batch_ind],taxon_probs)+(oh_index(locs_dynam,taxon_probs)*psi)\n",
    "            pseudo_z=self.z_transform(pseudo_z)\n",
    "            z_decoder_weight=self.zdw.guide_sample(s,scale=fest([pseudo_z.abs()],-1))\n",
    "            discov_dc=self.dc.guide_sample(s,scale=fest([discov,pseudo_z.abs()],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09348156-639f-409a-88f9-adf31c083670",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pyro.clear_param_store()\n",
    "    del antipode_model\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d011a60-4f3d-4ccf-a44d-4b52ed74b9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_var=adata.shape[1]\n",
    "batch_size=32\n",
    "level_sizes=[1,25,200]\n",
    "num_latent=150\n",
    "steps=0\n",
    "max_steps=80000\n",
    "print_every=5000\n",
    "\n",
    "# Clear Pyro param store so we don't conflict with previous run\n",
    "pyro.clear_param_store()\n",
    "# Fix random number seed to a lucky number\n",
    "pyro.util.set_rng_seed(13)\n",
    "# Enable optional validation warnings\n",
    "pyro.enable_validation(False)\n",
    "\n",
    "# Instantiate instance of model/guide and various neural networks\n",
    "antipode_model = ANTIPODE(num_latent=num_latent,level_sizes=level_sizes,\n",
    "                adata=adata,discov_pair=('obs','species'),batch_pair=('obs','batch'),layer='UMIs',\n",
    "                scale_init_val=0.01,loc_as_param=False,zdw_as_param=False,intercept_as_param=False,dist_normalize=True,bi_depth=2,\n",
    "                prior_scale=10.,num_batch_embed=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb275923-7fcb-40f2-bb95-d0f85a49d8e9",
   "metadata": {},
   "source": [
    "# Training Phase 1: Particlized tree approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6fe4dd-fdc5-4073-a2da-5758dd420841",
   "metadata": {},
   "outputs": [],
   "source": [
    "antipode_model.train_phase(phase=1,max_steps=max_steps,print_every=10000,num_particles=5,device=device, max_learning_rate=0.001, one_cycle_lr=True, steps=0, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48defcee-2f6f-4948-a68e-d7e5198fbf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5ce97b-ac83-4a30-bbd7-681dc9fe4e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(antipode_model.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c4066e-fd0d-4127-8a64-fe98b6f67530",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pyro.param('discov_int'))\n",
    "print(pyro.param('discov_mul'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebf3828-3ba9-438b-93ab-e409d8ba098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "antipode_model.store_outputs(device=device,prefix='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3526c3f-2542-4356-a439-8272bbd2a93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gmm_heatmaps(antipode_model)\n",
    "plot_d_hists(antipode_model)\n",
    "plot_batch_embedding_pca(antipode_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8138ba09-5b3b-434c-aa87-c6b26c4daf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "MDE_KEY = \"X_antipode_MDE\"\n",
    "adata.obsm[MDE_KEY] = clip_latent_dimensions(scvi.model.utils.mde(adata.obsm['X_antipode']),0.1)\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[\"antipode_cluster\"],legend_fontsize=6,legend_fontweight='normal',\n",
    "    legend_loc='on data',palette=sc.pl.palettes.godsnot_102\n",
    ")\n",
    "\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[\"species\",'batch'],palette=sc.pl.palettes.godsnot_102\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6ccfef-b178-4f5c-ada0-23a46148e854",
   "metadata": {},
   "source": [
    "# Training Phase 2: Inintializing categorical layered tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8574439f-d5d8-45be-bce4-b39f242afb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "antipode_model.prepare_phase_2(epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d84820f-7fdc-4aa1-85e4-131f52b24c35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "antipode_model.train_phase(phase=2,max_steps=max_steps,print_every=10000,num_particles=3,device=device, max_learning_rate=5e-4, one_cycle_lr=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa667753-46b5-4383-8985-aff80be975d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "antipode_model.pretrain_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df7ff3f-d191-4f68-ba09-cadb72a18dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(antipode_model.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f27369-70b8-44c2-8315-87c7363b4919",
   "metadata": {},
   "outputs": [],
   "source": [
    "antipode_model.store_outputs(device=device,prefix='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d0c19b-e1c0-4812-b302-1b31c066160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gmm_heatmaps(antipode_model)\n",
    "plot_d_hists(antipode_model)\n",
    "plot_tree_edge_weights(antipode_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9950f20-aa73-47b8-8226-129c6b1e5e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MDE_KEY = \"X_antipode_MDE\"\n",
    "adata.obsm[MDE_KEY] = clip_latent_dimensions(scvi.model.utils.mde(adata.obsm['X_antipode']),0.1)\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[\"antipode_cluster\",\"kmeans\"],legend_fontsize=6,legend_fontweight='normal',\n",
    "    legend_loc='on data',palette=sc.pl.palettes.godsnot_102\n",
    ")\n",
    "\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[\"species\"],palette=sc.pl.palettes.godsnot_102\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e8e92c-43c6-4887-8fcf-36f153010dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[x for x in adata.obs.columns if 'level' in x],\n",
    "    palette=sc.pl.palettes.godsnot_102,\n",
    "    legend_loc='on data'\n",
    ")\n",
    "\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[\"psi\"],\n",
    "    cmap='coolwarm'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7580e991-113c-4ca5-be71-875e36e88cba",
   "metadata": {},
   "source": [
    "# Training Phase 3: Refining the final tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ac89ea-a37e-4572-bed9-06cc79c66160",
   "metadata": {},
   "outputs": [],
   "source": [
    "antipode_model.train_phase(phase=3,max_steps=max_steps,print_every=10000,num_particles=3,device=device, max_learning_rate=1e-4, one_cycle_lr=True, steps=0, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31daac2e-e365-4d5a-928a-d641192bf861",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(antipode_model.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa427559-571b-4cd3-9101-1f4ca11e6ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "antipode_model.store_outputs(device=device,prefix='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef230dd1-cae7-426b-bcb1-9eaa2151da21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gmm_heatmaps(antipode_model)\n",
    "plot_d_hists(antipode_model)\n",
    "plot_tree_edge_weights(antipode_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956e0341-5311-4e46-8353-6bad02d86345",
   "metadata": {},
   "outputs": [],
   "source": [
    "MDE_KEY = \"X_antipode_MDE\"\n",
    "adata.obsm[MDE_KEY] = clip_latent_dimensions(scvi.model.utils.mde(adata.obsm['X_antipode']),0.1)\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[\"antipode_cluster\",\"kmeans\"],legend_fontsize=6,legend_fontweight='normal',\n",
    "    legend_loc='on data',palette=sc.pl.palettes.godsnot_102\n",
    ")\n",
    "\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[\"species\"],palette=sc.pl.palettes.godsnot_102\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76ba18-de62-4a4d-8cbc-4490c79e61c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[\"antipode_cluster\"],\n",
    "    palette=sc.pl.palettes.godsnot_102,\n",
    "    legend_loc='on data'\n",
    ")\n",
    "\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[x for x in adata.obs.columns if 'level' in x],\n",
    "    palette=sc.pl.palettes.godsnot_102,\n",
    "    legend_loc='on data'\n",
    ")\n",
    "\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[\"psi\",'species'],\n",
    "    cmap='coolwarm'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc02f99-05cd-49a9-a291-86de56a8f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.histplot(scipy.special.softmax( adata.uns['param_store']['edges_1'],-1).max(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be38d9ae-0fdc-4fb6-816e-4c1c0d8b39bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_choice=np.random.choice(adata.obs.index,size=100000,replace=False)\n",
    "random_choice=np.where(adata.obs.index.isin(random_choice))[0]\n",
    "xdata=adata[random_choice,:]\n",
    "xdata=xdata.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce1914e-0bd5-4a02-970e-c240122af343",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata.X=xdata.layers['UMIs'].copy()\n",
    "sc.pp.normalize_per_cell(xdata)\n",
    "sc.pp.log1p(xdata)\n",
    "#sc.pp.scale(xdata,max_value=10)\n",
    "\n",
    "gene_list=['RBFOX3','PDGFRA','AQP4','FOXJ1','AIF1','MOG','COL1A2','CD34','COL4A1','SATB2','RORB','DLX2','PROX1','SCGN','TSHZ1','SLC17A7','TLE4','FEZF2',\n",
    "           'MEIS2','NKX2-1','LHX6','CRABP1','TSHZ1','NPY','FOXP1','FOXP2','PDYN','PENK','ISL1','FOXG1','PDGFRA','AIF1','AQP4','EDNRB','FOXJ1','CD34','MKI67','RPL7','RPS17','RPL13A','MEF2C']\n",
    "gene_list=[x for x in gene_list if x in xdata.var.index]\n",
    "sc.pl.embedding(\n",
    "    xdata,\n",
    "    basis=MDE_KEY,\n",
    "    color=gene_list,cmap='Purples',\n",
    "    palette=sc.pl.palettes.godsnot_102,legend_fontsize=6,\n",
    "    legend_loc='on data',use_raw=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0501b16-f8f6-4b73-941a-ea43c52bbb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "means=group_aggr_anndata(xdata,['species'], agg_func=np.mean)\n",
    "for i in range(means[0].shape[0]):\n",
    "    seaborn.histplot(means[0][i,:], label=means[1]['species'][i])\n",
    "plt.legend()\n",
    "plt.title('Gene Means')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86342f7b-bd67-4a06-a60f-4c8bd7c3c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pyro.param('discov_mul'))\n",
    "print(pyro.param('discov_int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c02efa1-5972-4145-be3c-d383aff915e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata.X=xdata.raw.X[:,adata.raw.var.index.isin(adata.var.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604e73ca-8c08-4e5f-8c7c-3404ff7dfb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata.write_h5ad('/home/matthew.schmitz/Matthew/1.9.1.4_retina.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0a3fcd-9474-4ef2-980a-6de724fb430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xdata=sc.read_h5ad('/home/matthew.schmitz/Matthew/1.9.4_devbigger.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36573f63-de85-47b2-a0de-c581731381a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb85cef-b77b-4da5-a611-8c22e055e556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyro2",
   "language": "python",
   "name": "pyro2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
