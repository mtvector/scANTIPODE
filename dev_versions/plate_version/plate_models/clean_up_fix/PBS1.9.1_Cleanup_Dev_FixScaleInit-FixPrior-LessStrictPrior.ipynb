{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffe491f-acab-469c-90c9-4fb859e695fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# various import statements\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn import cluster\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import inspect\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import softplus, softmax\n",
    "from torch.distributions import constraints\n",
    "import seaborn\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "import pyro.optim\n",
    "from pyro.infer import SVI\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "sc.settings.figdir=os.path.expanduser('~/WbFigures/SpeciesDivergenceNoScaling')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55ede1a-5159-40ae-bb51-f03a129f7156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata=sc.read_h5ad(os.path.expanduser('/home/matthew.schmitz/Matthew/data/cortex_data/v1_combination.h5ad'),backed='r')\n",
    "# adata=adata[~adata.obs['dataset'].isin(['jorstad_cross_areal','krienen_marmoset']),:]\n",
    "# adata.write_h5ad(os.path.expanduser('/home/matthew.schmitz/Matthew/data/cortex_data/v1_combination_nojo.h5ad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a282e6-78c4-4e8a-8ac0-e1db80319c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata=sc.read_h5ad(os.path.expanduser('/allen/programs/celltypes/workgroups/rnaseqanalysis/EvoGen/Team/Matthew/data/taxtest/HvQvM/HvQvMall.h5ad'),backed='r')\n",
    "\n",
    "adata.obsm[\"X_original_umap\"]=adata.obsm[\"X_umap\"]\n",
    "sc.pl.umap(adata,color='species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371c61df-51ad-4d4b-b5b6-39fd5d1e4b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/matthew.schmitz/Matthew/code/scANTIPODE/antipode/')\n",
    "import antipode_model\n",
    "from antipode_model import *\n",
    "import model_functions\n",
    "from model_functions import *\n",
    "import model_distributions\n",
    "from model_distributions import *\n",
    "import model_modules\n",
    "from model_modules import *\n",
    "import train_utils\n",
    "from train_utils import *\n",
    "import plotting\n",
    "from plotting import *\n",
    "\n",
    "import importlib\n",
    "antipode_model=importlib.reload(antipode_model)\n",
    "from antipode_model import *\n",
    "\n",
    "import importlib\n",
    "model_modules=importlib.reload(model_modules)\n",
    "from model_modules import *\n",
    "\n",
    "model_functions=importlib.reload(model_functions)\n",
    "from model_functions import *\n",
    "\n",
    "import importlib\n",
    "model_distributions=importlib.reload(model_distributions)\n",
    "from model_distributions import *\n",
    "\n",
    "import importlib\n",
    "train_utils=importlib.reload(train_utils)\n",
    "from train_utils import *\n",
    "\n",
    "import importlib\n",
    "plotting=importlib.reload(plotting)\n",
    "from plotting import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a3ef7d-d676-44ad-b4d4-eade22dfc9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AntipodeTrainingMixin:\n",
    "    '''\n",
    "    Mixin class providing functions to actually run ANTIPODE\n",
    "    can use supervised particleomy by training only phase2\n",
    "    '''\n",
    "    \n",
    "    def save_params_to_uns(self,prefix=''):\n",
    "        pstore=param_store_to_numpy()\n",
    "        pstore={n:pstore[n] for n in pstore.keys() if not re.search('encoder|classifier',n)}\n",
    "        self.adata_manager.adata.uns[prefix+'param_store']=pstore\n",
    "\n",
    "    def get_antipode_outputs(self,batch_size=2048,device='cuda'):\n",
    "        design_matrix=False  #3x faster\n",
    "        \n",
    "        if self.discov_key not in self.adata_manager.adata.obsm.keys():\n",
    "            onehot_key=self.discov_key+\"_onehot\"\n",
    "            self.adata_manager.adata.obsm[onehot_key]=numpy_onehot( self.adata_manager.adata.obs[self.discov_key].cat.codes)\n",
    "        else:\n",
    "            onehot_key=self.discov_key\n",
    "        self.adata_manager.register_new_fields([scvi.data.fields.ObsmField(onehot_key,onehot_key)])       \n",
    "        field_types={\"s\":np.float32,onehot_key:np.float32}\n",
    "        dataloader=scvi.dataloaders.AnnDataLoader(self.adata_manager,batch_size=32,drop_last=False,shuffle=False,data_and_attributes=field_types)#supervised_field_types for supervised step \n",
    "        encoder_outs=batch_output_from_dataloader(dataloader,self.zl_encoder,batch_size=batch_size,device=device)\n",
    "        encoder_outs[0]=self.z_transform(encoder_outs[0])\n",
    "        encoder_out=[x.detach().cpu().numpy() for x in encoder_outs]\n",
    "        classifier_outs=batch_torch_outputs([encoder_outs[0]],self.classifier,batch_size=2048,device='cuda')\n",
    "        classifier_out=[x.detach().cpu().numpy() for x in classifier_outs]\n",
    "        return encoder_out,classifier_out   \n",
    "        \n",
    "    def store_outputs(self,device='cuda',prefix=''):\n",
    "        self.save_params_to_uns(prefix='')\n",
    "        self.to('cpu')\n",
    "        self.eval()\n",
    "        antipode_outs=get_antipode_outputs(self,batch_size=2048,device=device)\n",
    "        taxon=antipode_outs[1][0]\n",
    "        self.adata_manager.adata.obsm['X_antipode']=antipode_outs[0][0]\n",
    "        self.adata_manager.adata.obs['psi']=antipode_outs[1][1]\n",
    "        level_edges=[numpy_hardmax(self.adata_manager.adata.uns['param_store']['edges_'+str(i)],axis=-1) for i in range(len(self.level_sizes)-1)]\n",
    "        levels=self.tree_convergence_bottom_up.just_propagate(scipy.special.softmax(taxon[...,-self.level_sizes[-1]:],axis=-1),level_edges,s=torch.ones(1))\n",
    "        prop_taxon=np.concatenate(levels,axis=-1)\n",
    "        self.adata_manager.adata.obsm['taxon_probs']=prop_taxon\n",
    "        levels=self.tree_convergence_bottom_up.just_propagate(numpy_hardmax(levels[-1],axis=-1),level_edges,s=torch.ones(1))\n",
    "        for i in range(len(levels)):\n",
    "            cur_clust=prefix+'level_'+str(i)\n",
    "            self.adata_manager.adata.obs[cur_clust]=levels[i].argmax(1)\n",
    "            self.adata_manager.adata.obs[cur_clust]=self.adata_manager.adata.obs[cur_clust].astype(str)\n",
    "        self.adata_manager.adata.obs[prefix+'antipode_cluster'] = self.adata_manager.adata.obs.apply(lambda x: '_'.join([x[prefix+'level_'+str(i)] for i in range(len(levels))]), axis=1)\n",
    "\n",
    "\n",
    "    def fix_scale_factor(self,svi,x,ideal_val=0.01):\n",
    "        o1=svi.evaluate_loss(*x)\n",
    "        s1=self.scale_factor\n",
    "        s2=ideal_val*s1/o1\n",
    "        self.scale_factor=s2\n",
    "    \n",
    "    def train_phase_1(self,max_steps,print_every=10000,device='cuda',max_learning_rate=0.001,num_particles=3,one_cycle_lr=True,steps=0,batch_size=32):\n",
    "        #particle phase\n",
    "        steps=steps\n",
    "        print(self.fields)\n",
    "        print(self.field_types)\n",
    "        dataloader=scvi.dataloaders.AnnDataLoader(self.adata_manager,batch_size=32,drop_last=True,shuffle=True,data_and_attributes=self.field_types)#supervised_field_types for supervised step\n",
    "        scheduler=pyro.optim.OneCycleLR({'max_lr':max_learning_rate,'total_steps':max_steps,'div_factor':100,'optim_args':{},'optimizer':torch.optim.Adam}) if one_cycle_lr else pyro.optim.ClippedAdam({'lr':max_learning_rate,'lrd':(1-(5e-6))})\n",
    "        elbo = pyro.infer.JitTrace_ELBO(num_particles=num_particles,strict_enumeration_warning=False)\n",
    "        svi = SVI(self.model, self.guide, scheduler, elbo)\n",
    "        self.train()\n",
    "        self.zl_encoder.train()\n",
    "        \n",
    "        self=self.to(device)\n",
    "        self.set_approx(True)\n",
    "        self.losses=[]\n",
    "        pbar = tqdm.tqdm(total=max_steps, position=0)\n",
    "        done=False\n",
    "        while steps < max_steps:\n",
    "            for x in dataloader:\n",
    "                x['step']=torch.ones(1).to(device)*steps\n",
    "                x=[x[k].to(device) if k in x.keys() else torch.zeros(1) for k in self.args]\n",
    "                if self.scale_factor==1.:\n",
    "                    self.fix_scale_factor(svi,x)\n",
    "                loss=svi.step(*x)\n",
    "                steps+=1\n",
    "                if steps<max_steps-1:\n",
    "                    if one_cycle_lr:\n",
    "                        scheduler.step()\n",
    "                else:\n",
    "                    break\n",
    "                pbar.update(1)\n",
    "                self.losses.append(loss)\n",
    "                if steps%print_every == 0:\n",
    "                    # Tell the scheduler we've done one epoch.\n",
    "                    pbar.write(\"[Step %02d]  Loss: %.5f\" % (steps, np.mean(self.losses[-print_every:])))\n",
    "        \n",
    "        pbar.close()\n",
    "        allDone()\n",
    "        print(\"Finished training!\")\n",
    "        return(self.losses)\n",
    "\n",
    "    def prepare_phase_2(self):\n",
    "        '''Run this if not running in supervised only mode (JUST phase2 with provided obsm clustering), runs kmeans, resets tree edges and locs'''\n",
    "        kmeans = sklearn.cluster.MiniBatchKMeans(n_clusters=self.level_sizes[-1],init='k-means++',max_iter=1000,reassignment_ratio=0.001,n_init=100,random_state=0).fit(self.adata_manager.adata.obsm['X_antipode'])\n",
    "        self.adata_manager.adata.obs['kmeans']=kmeans.labels_\n",
    "        self.adata_manager.adata.obs['kmeans']=self.adata_manager.adata.obs['kmeans'].astype(int).astype('category')\n",
    "        self.adata_manager.adata.obsm['kmeans_onehot']=numpy_onehot(self.adata_manager.adata.obs['kmeans'].cat.codes,num_classes=self.level_sizes[-1]) #yoh=yoh+1e-10;yoh=oh/oh.sum(-1).reshape(-1,1)#for relaxed\n",
    "        new_locs=torch.concatenate(\n",
    "            [pyro.param('locs').new_zeros(sum(self.level_sizes[:-1]),pyro.param('locs').shape[1]),\n",
    "             torch.tensor(kmeans.cluster_centers_-kmeans.cluster_centers_.mean(0),device=pyro.param('locs').device)],\n",
    "             axis=0)\n",
    "        new_locs[0,:]=torch.tensor(kmeans.cluster_centers_.mean(0))\n",
    "        self.adata_manager.adata.obs['kmeans'].astype(int)\n",
    "        new_scales=group_aggr_anndata(self.adata_manager.adata,['kmeans'], agg_func=np.std,layer='X_antipode',obsm=True)[0]\n",
    "        new_scales=torch.concatenate(\n",
    "            [0.01*self.scale_init_val*new_locs.new_ones(sum(self.level_sizes[:-1]),pyro.param('locs').shape[1],requires_grad=True),\n",
    "             torch.tensor(new_scales+1e-10,device=pyro.param('scales').device,requires_grad=True)],axis=0).float()\n",
    "        self.adata_manager.adata.obs['kmeans'].astype(str)\n",
    "        pyro.get_param_store().__setitem__('locs',new_locs)\n",
    "        pyro.get_param_store().__setitem__('locs_dynam',new_locs.new_zeros(new_locs.shape))\n",
    "        pyro.get_param_store().__setitem__('scales',new_scales)\n",
    "        \n",
    "        for n in pyro.get_param_store():\n",
    "            if 'edge' in n:\n",
    "                pyro.get_param_store().__setitem__(n,pyro.param(n).new_zeros(pyro.param(n).shape))\n",
    "        self.adata_manager.adata.obs['kmeans']=self.adata_manager.adata.obs['kmeans'].astype(str)\n",
    "        \n",
    "    def train_phase_2(self,max_steps, taxon_label='kmeans_onehot', print_every=10000, device='cuda', max_learning_rate=0.001, num_particles=1, one_cycle_lr=False, steps=0, batch_size=32):\n",
    "        '''empirically works best and fastest with one_cycle_lr=False'''\n",
    "        steps=steps\n",
    "        supervised_field_types=self.field_types.copy()\n",
    "        supervised_fields=self.fields.copy()\n",
    "        supervised_field_types[\"taxon\"]=np.float32\n",
    "        self.adata_manager.register_new_fields([make_field('taxon',('obsm',taxon_label))])\n",
    "        class_dataloader=scvi.dataloaders.AnnDataLoader(self.adata_manager, batch_size=batch_size, drop_last=True, shuffle=True, data_and_attributes=supervised_field_types)\n",
    "        scheduler=pyro.optim.OneCycleLR({'max_lr':max_learning_rate,'total_steps':max_steps, 'div_factor':100,'optim_args':{},'optimizer':torch.optim.Adam}) if one_cycle_lr else pyro.optim.ClippedAdam({'lr':max_learning_rate,'lrd':(1-(5e-6))})\n",
    "        elbo = pyro.infer.JitTraceEnum_ELBO(num_particles=num_particles,strict_enumeration_warning=False)\n",
    "        svi = SVI(self.model, self.guide, scheduler, elbo)\n",
    "        \n",
    "        self.train()\n",
    "        self=self.to(device)\n",
    "        self.set_approx(False)\n",
    "        self.losses=[]\n",
    "        #for steps in range(max_steps):\n",
    "        pbar = tqdm.tqdm(total=max_steps, position=0)\n",
    "        done=False\n",
    "        while steps < max_steps:\n",
    "            for x in class_dataloader:\n",
    "                x['step']=torch.ones(1).to(device)*steps\n",
    "                x=[x[k].to(device) if k in x.keys() else torch.zeros(1) for k in self.args]\n",
    "                if self.scale_factor==1.:\n",
    "                    self.fix_scale_factor(svi,x)\n",
    "                loss=svi.step(*x)\n",
    "                steps+=1\n",
    "                if steps<=max_steps-1:\n",
    "                    if one_cycle_lr:\n",
    "                        scheduler.step()\n",
    "                    pass\n",
    "                else:\n",
    "                    break\n",
    "                pbar.update(1)\n",
    "                self.losses.append(loss)\n",
    "                if steps%print_every == 0:\n",
    "                    # Tell the scheduler we've done one epoch.\n",
    "                    pbar.write(\"[Step %02d]  Loss: %.5f\" % (steps, np.mean(self.losses[-print_every:])))\n",
    "        \n",
    "        pbar.close()\n",
    "        allDone()\n",
    "        print(\"Finished training!\")\n",
    "        return(self.losses)\n",
    "        \n",
    "    def train_phase_3(self,max_steps,print_every=10000,device='cuda',max_learning_rate=2e-5,num_particles=3,one_cycle_lr=True,steps=0,batch_size=32):\n",
    "        '''Too high of learning rate may cause model to explode and most clusters to collapse'''\n",
    "        steps=steps\n",
    "        dataloader=scvi.dataloaders.AnnDataLoader(self.adata_manager,batch_size=batch_size,drop_last=True,shuffle=True,data_and_attributes=self.field_types)#supervised_field_types for supervised step\n",
    "        scheduler=pyro.optim.OneCycleLR({'max_lr':max_learning_rate,'total_steps':max_steps,'div_factor':100,'optim_args':{},'optimizer':torch.optim.Adam}) if one_cycle_lr else pyro.optim.ClippedAdam({'lr':max_learning_rate,'lrd':(1-(5e-6))})\n",
    "        elbo = pyro.infer.JitTraceEnum_ELBO(num_particles=num_particles,strict_enumeration_warning=False)\n",
    "        svi = SVI(self.model, self.guide, scheduler, elbo)\n",
    "\n",
    "        self.losses=[]\n",
    "        self.train()\n",
    "        self=self.to(device)\n",
    "        self.set_approx(False)\n",
    "        \n",
    "        #for steps in range(max_steps):\n",
    "        pbar = tqdm.tqdm(total=max_steps, position=0)\n",
    "        done=False\n",
    "        while steps < max_steps:\n",
    "            for x in dataloader:\n",
    "                x['step']=torch.ones(1).to(device)*steps\n",
    "                x=[x[k].to(device) if k in x.keys() else torch.zeros(1) for k in self.args]\n",
    "                if self.scale_factor==1.:\n",
    "                    self.fix_scale_factor(svi,x)\n",
    "                loss=svi.step(*x)\n",
    "                steps+=1\n",
    "                if steps<max_steps-1:\n",
    "                    if one_cycle_lr:\n",
    "                        scheduler.step()\n",
    "                        pass\n",
    "                    pass\n",
    "                else:\n",
    "                    break\n",
    "                pbar.update(1)\n",
    "                self.losses.append(loss)\n",
    "                if steps%print_every == 0:\n",
    "                    # Tell the scheduler we've done one epoch.\n",
    "                    pbar.write(\"[Step %02d]  Loss: %.5f\" % (steps, np.mean(self.losses[-print_every:])))\n",
    "        \n",
    "        pbar.close()\n",
    "        allDone()\n",
    "        print(\"Finished training!\")\n",
    "        return(self.losses)\n",
    "\n",
    "\n",
    "class ANTIPODE(PyroBaseModuleClass,AntipodeTrainingMixin):#\n",
    "    '''\n",
    "    ANTIPODE (Single Cell Ancestral Node Taxonomy Inference by Parcellation of Differential Expression) \n",
    "    is a variational inference model developed for the simultaneous analysis (DE) and \n",
    "    categorization (taxonomy generation) of cell types across evolution (or now any covariate) using single-cell RNA-seq data.\n",
    "    \n",
    "\n",
    "    Parameters:\n",
    "    adata (AnnData): An AnnData object containing the single-cell dataset.\n",
    "    discov_pair (tuple): A tuple indicating the key and location of the discovery covariate \n",
    "                         in the AnnData object. Format: ('key', 'location'), where location is \n",
    "                         either 'obs' or 'obsm'.\n",
    "    batch_pair (tuple): A tuple indicating the key and location of the batch covariate \n",
    "                        in the AnnData object. Format: ('key', 'location'), where location is \n",
    "                        either 'obs' or 'obsm'.\n",
    "    num_var (int): Number of variables (features) in the dataset.\n",
    "    level_sizes (list of int, optional): Sizes of each level in the model's hierarchical structure. \n",
    "                                         Defaults to [1, 10, 25, 50].\n",
    "    num_latent (int, optional): Number of latent dimensions. Defaults to 50.\n",
    "    scale_factor (float, optional): Scaling factor for data normalization. If None, it is inferred from the data.\n",
    "    prior_scale (float, optional): Scale of the Laplace prior distributions. Defaults to 100.\n",
    "    dcd_prior (float, optional): Scale of the prior for the decoder. If None, defaults to a specific inferred value.\n",
    "    theta_prior (float, optional): Init value for the inverse dispersion of the negative binomial.\n",
    "    decay_function (callable, optional): A function that defines the decay of certain parameters over iterations.\n",
    "    max_strictness (float, optional): Maximum strictness parameter for tree convergence. Defaults to 1.\n",
    "    bi_depth (int, optional): Depth of the tree for the approximation of batch by identity effects. Defaults to 2.\n",
    "    num_batch_embed (int, optional): Number of batch embeddings. Defaults to 10.\n",
    "    classifier_hidden (list of int, optional): Sizes of hidden layers for the classifier network. Defaults to [3000, 3000, 3000].\n",
    "    encoder_hidden (list of int, optional): Sizes of hidden layers for the encoder network. Defaults to [6000, 5000, 3000, 1000].\n",
    "    '''\n",
    "    def __init__(self, adata, discov_pair, batch_pair, layer, level_sizes=[1, 10, 25, 50], \n",
    "                 num_latent=50, scale_factor=None, prior_scale=100,dcd_prior=None,use_psi=True,loc_as_param=True,zdw_as_param=True,intercept_as_param=True,\n",
    "                 decay_function=None, max_strictness=1., bi_depth=2, num_batch_embed=10,theta_prior=50.,scale_init_val=0.01,\n",
    "                 classifier_hidden=[3000,3000,3000],encoder_hidden=[6000,5000,3000,1000],phase1_treeest=False,z_transform=None):\n",
    "\n",
    "        pyro.clear_param_store()\n",
    "\n",
    "        # Determine num_discov and num_batch from the AnnData object\n",
    "        self.discov_loc, self.discov_key = discov_pair\n",
    "        self.batch_loc, self.batch_key = batch_pair\n",
    "        self.num_discov = adata.obsm[self.discov_key].shape[-1] if self.discov_loc == 'obsm' else len(adata.obs[self.discov_key].unique())\n",
    "        self.num_batch = adata.obsm[self.batch_key].shape[-1] if self.batch_loc == 'obsm' else len(adata.obs[self.batch_key].unique())\n",
    "        self.design_matrix = (self.discov_loc == 'obsm')\n",
    "        self.layer=layer\n",
    "\n",
    "        self._setup_adata_manager_store: dict[str, type[scvi.data.AnnDataManager]] = {}\n",
    "        self.num_var = adata.layers[layer].shape[-1]\n",
    "        self.num_latent = num_latent\n",
    "        self.level_sizes = level_sizes\n",
    "        self.num_labels = np.sum(self.level_sizes)\n",
    "        self.level_indices = np.cumsum([0] + self.level_sizes)\n",
    "        self.bi_depth = bi_depth\n",
    "        self.num_bi_depth = sum(self.level_sizes[:self.bi_depth])\n",
    "        self.num_batch_embed = num_batch_embed\n",
    "        self.max_strictness = 1.\n",
    "        self.scale_factor = 1. if scale_factor is None else scale_factor\n",
    "        self.decay_function = gen_linear_function(2,1) if decay_function is None else decay_function \n",
    "        self.temperature = 0.1\n",
    "        self.epsilon = 0.006\n",
    "        self.approx = False\n",
    "        self.prior_scale = prior_scale\n",
    "        self.use_psi=use_psi\n",
    "        self.loc_as_param=loc_as_param\n",
    "        self.zdw_as_param=zdw_as_param\n",
    "        self.theta_prior=theta_prior\n",
    "        self.phase1_treeest=phase1_treeest\n",
    "        self.scale_init_val=scale_init_val\n",
    "        \n",
    "        self.dcd_prior=torch.zeros((self.num_discov,self.num_var)) if dcd_prior is None else dcd_prior#Use this for \n",
    "        \n",
    "        # Initialize plates to be used during sampling\n",
    "        self.var_plate=pyro.plate('var_plate',self.num_var,dim=-1)\n",
    "        self.discov_plate=pyro.plate('discov_plate',self.num_discov,dim=-3)\n",
    "        self.batch_plate=pyro.plate('batch_plate',self.num_batch,dim=-3)\n",
    "        self.latent_plate=pyro.plate('latent_plate',self.num_latent,dim=-1)\n",
    "        self.latent_plate2=pyro.plate('latent_plate2',self.num_latent,dim=-2)\n",
    "        self.label_plate=pyro.plate('label_plate',self.num_labels,dim=-2)\n",
    "        self.batch_embed_plate=pyro.plate('batch_embed_plate',self.num_batch_embed,dim=-3)\n",
    "        self.bi_depth_plate=pyro.plate('bi_depth_plate',self.num_bi_depth,dim=-2)\n",
    "\n",
    "        #Initialize MAP inference modules\n",
    "        self.dm=MAPLaplaceModule(self,'discov_dm',[self.num_discov,self.num_labels,self.num_latent],[self.discov_plate,self.label_plate,self.latent_plate])\n",
    "        self.bm=MAPLaplaceModule(self,'batch_dm',[self.num_batch,self.num_labels,self.num_latent],[self.batch_plate,self.label_plate,self.latent_plate])\n",
    "        self.di=MAPLaplaceModule(self,'discov_di',[self.num_discov,self.num_labels,self.num_var],[self.discov_plate,self.label_plate,self.var_plate])\n",
    "        self.bei=MAPLaplaceModule(self,'batch_di',[self.num_batch_embed,self.num_bi_depth,self.num_var],[self.batch_embed_plate,self.bi_depth_plate,self.var_plate])\n",
    "        self.ci=MAPLaplaceModule(self,'cluster_intercept',[self.num_labels, self.num_var],[self.label_plate,self.var_plate],param_only=intercept_as_param)\n",
    "        self.dc=MAPLaplaceModule(self,'discov_dc',[self.num_discov,self.num_latent,self.num_var],[self.discov_plate,self.latent_plate2,self.var_plate])\n",
    "        self.zdw=MAPLaplaceModule(self,'z_decoder_weight',[self.num_latent,self.num_var],[self.latent_plate2,self.var_plate],init_val=((2/self.num_latent)*(torch.rand(self.num_latent,self.num_var)-0.5)),param_only=self.zdw_as_param)\n",
    "        self.zl=MAPLaplaceModule(self,'locs',[self.num_labels,self.num_latent],[self.label_plate,self.latent_plate],param_only=self.loc_as_param)\n",
    "        self.zs=MAPLaplaceModule(self,'scales',[self.num_labels,self.num_latent],[self.label_plate,self.latent_plate],init_val=self.scale_init_val*torch.ones(self.num_labels,self.num_latent),constraint=constraints.positive,param_only=False)\n",
    "        self.zld=MAPLaplaceModule(self,'locs_dynam',[self.num_labels,self.num_latent],[self.label_plate,self.latent_plate],param_only=False)\n",
    "        \n",
    "        self.tree_edges=TreeEdges(self,straight_through=True)\n",
    "        self.tree_convergence=TreeConvergence(self,strictness=1.)        \n",
    "        self.tree_convergence_bottom_up=TreeConvergenceBottomUp(self,strictness=1.)        \n",
    "        self.z_transform=null_function if z_transform is None else z_transform#centered_sigmoid#torch.special.expit\n",
    "\n",
    "        if self.design_matrix:\n",
    "            fields={'s':('layers',self.layer),\n",
    "            'discov_ind':('obsm',self.discov_key),\n",
    "            'batch_ind':('obsm',self.batch_key)}\n",
    "            field_types={\"s\":np.float32,\"batch_ind\":np.float32,\"discov_ind\":np.float32}\n",
    "        else:\n",
    "            fields={'s':('layers',self.layer),\n",
    "            'discov_ind':('obs',self.discov_key),\n",
    "            'batch_ind':('obs',self.batch_key)}\n",
    "            field_types={\"s\":np.float32,\"batch_ind\":np.int64,\"discov_ind\":np.int64}\n",
    "\n",
    "        self.fields=fields\n",
    "        self.field_types=field_types\n",
    "        self.setup_anndata(adata, {'discov_ind': discov_pair, 'batch_ind': batch_pair}, self.field_types)\n",
    "        \n",
    "        super().__init__()\n",
    "        # Setup the various neural networks used in the model and guide\n",
    "        self.z_decoder=ZDecoder(num_latent=self.num_latent, num_var=self.num_var, hidden_dims=[])        \n",
    "        self.zl_encoder=ZLEncoder(num_var=self.num_var,hidden_dims=encoder_hidden,num_cat_input=self.num_discov,\n",
    "                    outputs=[(self.num_latent,None),(self.num_latent,softplus)])\n",
    "        \n",
    "        self.classifier=Classifier(num_latent=self.num_latent,hidden_dims=classifier_hidden,\n",
    "                    outputs=[(self.num_labels,None),(1,None),(1,softplus)])\n",
    "\n",
    "        #self.bc_nn=SimpleFFNN(in_dim=self.num_batch,hidden_dims=[200,200,50,5],\n",
    "        #            out_dim=self.num_var*self.num_latent)\n",
    "        #Too large to exactly model gene-level batch effects for all cluster x batch\n",
    "        self.be_nn=SimpleFFNN(in_dim=self.num_batch,hidden_dims=[1000,500,500],\n",
    "                    out_dim=self.num_batch_embed)\n",
    "        \n",
    "        self.epsilon = 0.006\n",
    "        #Initialize model in approximation mode\n",
    "        self.approx=False\n",
    "        self.prior_scale=prior_scale\n",
    "        self.args=inspect.getfullargspec(self.model).args[1:]#skip self\n",
    "\n",
    "    def setup_anndata(self,adata: anndata.AnnData,fields,field_types,**kwargs,):\n",
    "        \n",
    "        anndata_fields=[make_field(x,self.fields[x]) for x in self.fields.keys()]\n",
    "        \n",
    "        adata_manager = scvi.data.AnnDataManager(\n",
    "            fields=anndata_fields\n",
    "        )\n",
    "        adata_manager.register_fields(adata, **kwargs)\n",
    "        self.register_manager(adata_manager)\n",
    "        if fields['discov_ind'][0]=='obsm':\n",
    "            self.design_matrix=True\n",
    "            if fields['batch_ind'][0]!='obsm':\n",
    "                raise Exception(\"If discov is design matrix, batch must be as well!\")\n",
    "\n",
    "\n",
    "    def register_manager(self, adata_manager: scvi.data.AnnDataManager):\n",
    "        adata_id = adata_manager.adata_uuid\n",
    "        self._setup_adata_manager_store[adata_id] = adata_manager\n",
    "        self.adata_manager=adata_manager\n",
    "    \n",
    "    def set_approx(self,b: bool):\n",
    "        self.approx=b\n",
    "        \n",
    "    def model(self, s,discov_ind=torch.zeros(1),batch_ind=torch.zeros(1),step=torch.ones(1),taxon=torch.zeros(1)):\n",
    "        # Register various nn.Modules (i.e. the decoder/encoder networks) with Pyro\n",
    "        pyro.module(\"antipode\", self)\n",
    "\n",
    "        if not self.design_matrix:\n",
    "            batch=index_to_onehot(batch_ind,[s.shape[0],self.num_batch]).to(s.device)\n",
    "            discov=index_to_onehot(discov_ind,[s.shape[0],self.num_discov]).to(s.device)\n",
    "            batch_ind=batch_ind.squeeze()\n",
    "            discov_ind=discov_ind.squeeze()\n",
    "        else:\n",
    "            batch=batch_ind\n",
    "            discov=discov_ind\n",
    "        \n",
    "        minibatch_plate=pyro.plate(\"minibatch_plate\", s.shape[0],dim=-1)\n",
    "        minibatch_plate2=pyro.plate(\"minibatch_plate2\", s.shape[0],dim=-2)\n",
    "        cur_strictness=self.decay_function(step, self.max_strictness)\n",
    "        l = s.sum(1).unsqueeze(-1)\n",
    "        \n",
    "        # We scale all sample statements by scale_factor so that the ELBO loss function\n",
    "        # is normalized wrt the number of datapoints and genes.\n",
    "        # This helps with numerical stability during optimization.\n",
    "        with poutine.scale(scale=self.scale_factor):\n",
    "            # This gene-level parameter modulates the variance of the observation distribution\n",
    "            s_theta = pyro.param(\"s_inverse_dispersion\", self.theta_prior * s.new_ones(self.num_var),\n",
    "                               constraint=constraints.positive)\n",
    "            \n",
    "            dcd=pyro.param(\"discov_constitutive_de\", self.dcd_prior.to(s.device))\n",
    "            level_edges=self.tree_edges.model_sample(s,approx=self.approx)\n",
    "            \n",
    "            with minibatch_plate:\n",
    "                beta_prior_a=1.*s.new_ones(self.num_labels)\n",
    "                beta_prior_a[0]=10.\n",
    "                if self.approx:#Bernoulli particles approx?\n",
    "                    taxon_probs = pyro.sample(\"taxon_probs\", dist.Beta(beta_prior_a,s.new_ones(self.num_labels),validate_args=True).to_event(1))\n",
    "                    taxon = pyro.sample('taxon',dist.RelaxedBernoulli(temperature=0.1*s.new_ones(1),probs=taxon_probs).to_event(1))\n",
    "                    if self.phase1_treeest:\n",
    "                        self.tree_convergence.model_sample(taxon,level_edges,s,cur_strictness)#Someday will be possible to properly generate Undirected acyclic graphs here\n",
    "                else:\n",
    "                    taxon_probs=pyro.sample('taxon_probs',dist.Dirichlet(s.new_ones(s.shape[0],self.level_sizes[-1]),validate_args=True))\n",
    "                    if sum(taxon.shape) > 1:#Supervised?\n",
    "                        if taxon.shape[-1]==self.num_labels:#Totally supervised?\n",
    "                            pass\n",
    "                        else:#Only bottom layer is supervised?\n",
    "                            taxon = pyro.sample(\"taxon\", dist.OneHotCategorical(probs=taxon_probs,validate_args=True),obs=taxon)\n",
    "                            taxon = torch.concat(self.tree_convergence_bottom_up.just_propagate(taxon,level_edges,s),-1)\n",
    "                    else:#Unsupervised\n",
    "                        taxon = pyro.sample(\"taxon\", dist.OneHotCategorical(logits=s.new_zeros(s.shape[0],self.level_sizes[-1]),validate_args=True),infer={'enumerate':'parallel'})\n",
    "                        taxon = torch.concat(self.tree_convergence_bottom_up.just_propagate(taxon,level_edges,s),-1)\n",
    "                    taxon_probs=self.tree_convergence_bottom_up.just_propagate(taxon_probs,level_edges,s,cur_strictness)\n",
    "                    taxon_probs=torch.cat(taxon_probs,-1)\n",
    "                   \n",
    "            locs=self.zl.model_sample(s,scale=fest([taxon_probs],-1))\n",
    "            scales=self.zs.model_sample(s,scale=fest([taxon_probs],-1))\n",
    "            locs_dynam=self.zld.model_sample(s,scale=fest([taxon_probs],-1))\n",
    "            discov_dm=self.dm.model_sample(s,scale=fest([discov,taxon_probs],-1))\n",
    "            discov_di=self.di.model_sample(s,scale=fest([discov,taxon_probs],-1))\n",
    "            batch_dm=self.bm.model_sample(s,scale=fest([batch,taxon_probs],-1))\n",
    "            batch_embed=centered_sigmoid(self.be_nn(batch))\n",
    "            bei=self.bei.model_sample(s,scale=fest([batch_embed,taxon_probs[...,:self.num_bi_depth]],-1))\n",
    "            cluster_intercept=self.ci.model_sample(s,scale=fest([taxon_probs],-1))\n",
    "            \n",
    "            with minibatch_plate:\n",
    "                bi=torch.einsum('...bi,...ijk->...bjk',batch_embed,bei)\n",
    "                bi=torch.einsum('...bj,...bjk->...bk',taxon[...,:self.num_bi_depth],bi)\n",
    "                psi = centered_sigmoid(pyro.sample('psi',dist.Laplace(s.new_zeros(s.shape[0],1),self.prior_scale*s.new_ones(s.shape[0],1)).to_event(1)))\n",
    "                psi = 0 if not self.use_psi else psi\n",
    "                this_locs=oh_index(locs,taxon)\n",
    "                this_scales=oh_index(scales,taxon)\n",
    "                z=pyro.sample('z', dist.Normal(this_locs,this_scales+self.epsilon,validate_args=True).to_event(1))\n",
    "\n",
    "            cur_discov_dm = oh_index1(discov_dm, discov_ind) if self.design_matrix else discov_dm[discov_ind]\n",
    "            cur_batch_dm = oh_index1(batch_dm, batch_ind) if self.design_matrix else batch_dm[batch_ind]\n",
    "            cur_dcd = oh_index(dcd, discov) if self.design_matrix else  dcd[discov_ind]\n",
    "            \n",
    "            z=z+oh_index2(cur_discov_dm,taxon) + oh_index2(cur_batch_dm,taxon)+(oh_index(locs_dynam,taxon)*psi)\n",
    "            z=self.z_transform(z)                \n",
    "            fake_z=oh_index(locs,taxon_probs)+oh_index2(discov_dm[discov_ind],taxon_probs) + oh_index2(batch_dm[batch_ind],taxon_probs)+(oh_index(locs_dynam,taxon_probs)*psi)\n",
    "            fake_z=self.z_transform(fake_z)\n",
    "            z_decoder_weight=self.zdw.model_sample(s,scale=fest([fake_z.abs()],-1))\n",
    "            discov_dc=self.dc.model_sample(s,scale=fest([discov,fake_z.abs()],-1))\n",
    "            cur_discov_di = oh_index1(discov_di, discov_ind) if self.design_matrix else discov_di[discov_ind]\n",
    "            cur_discov_dc = oh_index1(discov_dc, discov_ind) if self.design_matrix else discov_dc[discov_ind]\n",
    "            cur_discov_di=oh_index2(cur_discov_di,taxon)\n",
    "            cur_cluster_intercept=oh_index(cluster_intercept,taxon)\n",
    "            \n",
    "            mu=torch.einsum('...bi,...bij->...bj',z,z_decoder_weight+cur_discov_dc)#+bc\n",
    "            spliced_mu=mu+cur_dcd+cur_discov_di+cur_cluster_intercept+bi\n",
    "            spliced_out=torch.softmax(spliced_mu,dim=-1)\n",
    "            log_mu = (l * spliced_out + 1e-6).log()\n",
    "            \n",
    "            with self.var_plate,minibatch_plate2:\n",
    "                s_dist = dist.NegativeBinomial(total_count=s_theta,logits=log_mu-s_theta.log(),validate_args=True)\n",
    "                s_out=pyro.sample(\"s\", s_dist, obs=s.int())\n",
    "\n",
    "    \n",
    "    # The guide specifies the variational distribution\n",
    "    def guide(self, s,discov_ind=torch.zeros(1),batch_ind=torch.zeros(1),step=torch.ones(1),taxon=torch.zeros(1)):\n",
    "        pyro.module(\"antipode\", self)\n",
    "        \n",
    "        if not self.design_matrix:\n",
    "            batch=index_to_onehot(batch_ind,[s.shape[0],self.num_batch]).to(s.device)\n",
    "            discov=index_to_onehot(discov_ind,[s.shape[0],self.num_discov]).to(s.device)\n",
    "            batch_ind=batch_ind.squeeze()\n",
    "            discov_ind=discov_ind.squeeze()\n",
    "        else:\n",
    "            batch=batch_ind\n",
    "            discov=discov_ind\n",
    "        \n",
    "        minibatch_plate=pyro.plate(\"minibatch_plate\", s.shape[0])\n",
    "        cur_strictness=self.decay_function(step, self.max_strictness)\n",
    "        \n",
    "        with poutine.scale(scale=self.scale_factor):\n",
    "            level_edges=self.tree_edges.guide_sample(s,approx=self.approx) \n",
    "            with minibatch_plate:\n",
    "                z_loc, z_scale= self.zl_encoder(s,discov)\n",
    "                z=pyro.sample('z',dist.Normal(z_loc,z_scale+self.epsilon).to_event(1))\n",
    "                pyro.sample('z_cost',dist.Delta(z_loc).to_event(1))\n",
    "                z=self.z_transform(z)\n",
    "                taxon_logits,psi_loc,psi_scale=self.classifier(z)\n",
    "                psi=centered_sigmoid(pyro.sample('psi',dist.Normal(psi_loc,psi_scale).to_event(1)))\n",
    "                psi = 0 if not self.use_psi else psi\n",
    "                if self.approx:\n",
    "                    taxon_dist = dist.Delta(safe_sigmoid(taxon_logits),validate_args=True).to_event(1)\n",
    "                    taxon_probs = pyro.sample(\"taxon_probs\", taxon_dist)\n",
    "                    taxon = pyro.sample('taxon',dist.RelaxedBernoulli(temperature=self.temperature*s.new_ones(1),probs=taxon_probs).to_event(1))\n",
    "                    if self.phase1_treeest:\n",
    "                        self.tree_convergence.guide_sample(taxon,level_edges,s,cur_strictness)\n",
    "                else:\n",
    "                    taxon_probs=pyro.sample('taxon_probs',dist.Delta(safe_softmax(taxon_logits[...,-self.level_sizes[-1]:])).to_event(1))\n",
    "                    if sum(taxon.shape) > 1:\n",
    "                        pass\n",
    "                    else:\n",
    "                        taxon = pyro.sample(\"taxon\", \n",
    "                                         dist.OneHotCategorical(probs=taxon_probs,validate_args=True),infer={'enumerate':'parallel'})                    \n",
    "                    if taxon.shape[-1]<self.num_labels:\n",
    "                        taxon = torch.concat(self.tree_convergence_bottom_up.just_propagate(taxon,level_edges,s),-1)\n",
    "\n",
    "                    taxon_probs=self.tree_convergence_bottom_up.just_propagate(taxon_probs[...,-self.level_sizes[-1]:],level_edges,s,cur_strictness)\n",
    "                    taxon_probs=torch.cat(taxon_probs,-1)\n",
    "            locs=self.zl.guide_sample(s,scale=fest([taxon_probs],-1))\n",
    "            scales=self.zs.guide_sample(s,scale=fest([taxon_probs],-1))\n",
    "            locs_dynam=self.zld.guide_sample(s,scale=fest([taxon_probs],-1))\n",
    "            discov_dm=self.dm.guide_sample(s,scale=fest([discov,taxon_probs],-1))\n",
    "            batch_dm=self.bm.guide_sample(s,scale=fest([batch,taxon_probs],-1))\n",
    "            batch_embed=centered_sigmoid(self.be_nn(batch))\n",
    "            discov_di=self.di.guide_sample(s,scale=fest([discov,taxon_probs],-1))\n",
    "            cluster_intercept=self.ci.guide_sample(s,scale=fest([taxon_probs],-1))\n",
    "            bei=self.bei.guide_sample(s,scale=fest([batch_embed,taxon_probs[...,:self.num_bi_depth]],-1))#maybe should be abs sum bei\n",
    "            if self.design_matrix:\n",
    "                z=z+oh_index2(oh_index1(discov_dm,discov_ind),taxon) + oh_index2(oh_index1(batch_dm,batch_ind),taxon)+(oh_index(locs_dynam,taxon)*psi)\n",
    "            else:\n",
    "                z=z+oh_index2(discov_dm[discov_ind],taxon) + oh_index2(batch_dm[batch_ind],taxon)+(oh_index(locs_dynam,taxon)*psi)\n",
    "            z=self.z_transform(z)\n",
    "            fake_z=oh_index(locs,taxon_probs)+oh_index2(discov_dm[discov_ind],taxon_probs) + oh_index2(batch_dm[batch_ind],taxon_probs)+(oh_index(locs_dynam,taxon_probs)*psi)\n",
    "            fake_z=self.z_transform(fake_z)\n",
    "            z_decoder_weight=self.zdw.guide_sample(s,scale=fest([fake_z.abs()],-1))\n",
    "            discov_dc=self.dc.guide_sample(s,scale=fest([discov,fake_z.abs()],-1))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8188b4f-93d3-417e-b6d6-061bdb88f6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pyro.clear_param_store()\n",
    "    del antipode_model\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3438c5-dd7e-41aa-bde8-2183986f0bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_var=adata.shape[1]\n",
    "batch_size=32\n",
    "level_sizes=[1,20,50,200]\n",
    "num_latent=200\n",
    "num_particles=sum(level_sizes)\n",
    "steps=0\n",
    "max_steps=120000\n",
    "print_every=5000\n",
    "\n",
    "# Clear Pyro param store so we don't conflict with previous run\n",
    "pyro.clear_param_store()\n",
    "# Fix random number seed to a lucky number\n",
    "pyro.util.set_rng_seed(13)\n",
    "# Enable optional validation warnings\n",
    "pyro.enable_validation(False)\n",
    "\n",
    "decay_function=gen_linear_function(max_steps,10000)#gen_exponential_decay(5e-6)\n",
    "\n",
    "# Instantiate instance of model/guide and various neural networks\n",
    "antipode_model = ANTIPODE(num_latent=num_latent,level_sizes=level_sizes,\n",
    "                adata=adata,discov_pair=('obs','species'),batch_pair=('obs','batch_name'),layer='spliced',loc_as_param=True,zdw_as_param=True,intercept_as_param=True,\n",
    "                bi_depth=3,decay_function=decay_function,max_strictness=1,prior_scale=100.,num_batch_embed=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c181029a-beb4-4d05-bdda-3854b8fda660",
   "metadata": {},
   "source": [
    "# Training Phase 1: Particlized tree approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d7be17-ca10-42bd-9b6a-deacd43f59aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "antipode_model.train_phase_1(max_steps=max_steps,print_every=10000,num_particles=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df09d64-4b95-4eb8-b5db-97943a845336",
   "metadata": {},
   "outputs": [],
   "source": [
    "antipode_model.store_outputs(device=device,prefix='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0250db4c-2c28-4fcd-bcdb-059d8810cf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gmm_heatmaps(antipode_model)\n",
    "plot_tree_edge_weights(antipode_model)\n",
    "plot_d_hists(antipode_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ce8d82-5d92-4eb7-9abd-12c47f1379f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MDE_KEY = \"X_antipode_MDE\"\n",
    "adata.obsm[MDE_KEY] = scvi.model.utils.mde(adata.obsm['X_antipode'])\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[\"antipode_cluster\"],legend_fontsize=6,legend_fontweight='normal',\n",
    "    legend_loc='on data',palette=sc.pl.palettes.godsnot_102\n",
    ")\n",
    "\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[\"species\"],palette=sc.pl.palettes.godsnot_102\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de70cb5-08f1-4fc1-86ac-dc777e7615b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "random_choice=np.random.choice(adata.obs.index,size=20000,replace=False)\n",
    "random_choice=np.where(adata.obs.index.isin(random_choice))[0]\n",
    "xdata=adata[random_choice,:]\n",
    "xdata=xdata.to_memory().copy()\n",
    "\n",
    "sc.pp.neighbors(xdata,n_neighbors=20, use_rep=\"X_antipode\")\n",
    "sc.tl.umap(xdata)\n",
    "sc.pl.umap(xdata,color=['species'],use_raw=False,palette=sc.pl.palettes.godsnot_102)\n",
    "sc.pl.umap(xdata,color=['psi'],use_raw=False,legend_loc='on data',cmap='coolwarm')\n",
    "sc.pl.umap(xdata,color=['antipode_cluster'],use_raw=False,legend_loc=None,palette=sc.pl.palettes.godsnot_102)\n",
    "\"\"\"\n",
    "# xdata.X=xdata.layers['UMIs']\n",
    "# sc.pp.normalize_per_cell(xdata)\n",
    "# sc.pp.log1p(xdata)\n",
    "# sc.pp.scale(xdata,max_value=10)\n",
    "#sc.pl.umap(xdata,color=['GBX2','EOMES','SIX3','OTX2','FOXG1','RBFOX3','TH','PDGFRA','AQP4','FOXJ1','AIF1','TTR','MOG','COL1A2','CD34','COL4A1','NPY','NKX2-1','FOXP2','SATB2','RORB','FEZF2','EMX1'],use_raw=False,cmap='Purples')\n",
    "#sc.pl.umap(xdata,color=['DLX2','PROX1','SCGN','TSHZ1','MEIS2','NKX2-1','LHX6','CRABP1','TSHZ1','FOXG1','PDGFRA','AIF1','AQP4','EDNRB','FOXJ1','CD34','MKI67'],cmap='Purples',use_raw=False)\n",
    "#sc.pl.umap(xdata,color=['RPL7','RPS17','RPL13A','MEF2C'],cmap='Purples',use_raw=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd4ded4-2f42-456e-bbc8-9d5c0d2b178b",
   "metadata": {},
   "source": [
    "# Training Phase 2: Inintializing categorical layered tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b051283f-c330-439d-95f5-ad7ae9c9d318",
   "metadata": {},
   "outputs": [],
   "source": [
    "antipode_model.prepare_phase_2()\n",
    "#sc.pl.umap(adata,color=['kmeans'],legend_loc=\"on data\",palette=sc.pl.palettes.godsnot_102)\n",
    "adata.obs['kmeans']=adata.obs['kmeans'].astype('category')\n",
    "# seaborn.clustermap(pyro.param('locs').detach().cpu().numpy())\n",
    "# plt.show()\n",
    "# seaborn.clustermap(pyro.param('locs_dynam').detach().cpu().numpy())\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8235e252-7d5b-4d16-9ff0-02d2e8b5f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.clustermap(torch.sigmoid(pyro.param('locs')).cpu().detach().numpy())\n",
    "plt.show()\n",
    "seaborn.clustermap(pyro.param('scales').cpu().detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346d710b-c5b3-460d-9c29-12f4f077043f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "antipode_model.train_phase_2(max_steps=max_steps,print_every=10000,num_particles=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c60c6d-62ec-4017-8132-4e93f56e3794",
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.clustermap(torch.sigmoid(pyro.param('locs')).cpu().detach().numpy())\n",
    "plt.show()\n",
    "seaborn.clustermap(pyro.param('scales').cpu().detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f01c405-4e45-41e8-b8ce-578b47e68907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seaborn.clustermap(torch.sigmoid(pyro.param('taxon_particles')).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2972a5-7bbe-466d-8593-e1094bcd9a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "antipode_model.store_outputs(device=device,prefix='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c3c958-b69f-4b1f-ac3b-c6c45e8f4bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gmm_heatmaps(antipode_model)\n",
    "plot_tree_edge_weights(antipode_model)\n",
    "plot_d_hists(antipode_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a782b8-a259-4f0c-8eba-dc8bd7e8295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MDE_KEY = \"X_antipode_MDE\"\n",
    "adata.obsm[MDE_KEY] = scvi.model.utils.mde(adata.obsm['X_antipode'])\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[\"antipode_cluster\",\"kmeans\"],legend_fontsize=6,legend_fontweight='normal',\n",
    "    legend_loc='on data',palette=sc.pl.palettes.godsnot_102\n",
    ")\n",
    "\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[\"species\"],palette=sc.pl.palettes.godsnot_102\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de1cd5d-7a88-4052-8ad5-2354b786cc42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "random_choice=np.random.choice(adata.obs.index,size=10000,replace=False)\n",
    "random_choice=np.where(adata.obs.index.isin(random_choice))[0]\n",
    "xdata=adata[random_choice,:]\n",
    "xdata=xdata.to_memory().copy()\n",
    "sc.pp.neighbors(xdata,n_neighbors=20, use_rep=\"X_antipode\")\n",
    "sc.tl.umap(xdata)\n",
    "sc.pl.umap(xdata,color=['species'],use_raw=False,legend_loc='on data',palette=sc.pl.palettes.godsnot_102)\n",
    "sc.pl.umap(xdata,color=['psi'],use_raw=False,legend_loc='on data',cmap='coolwarm')\n",
    "sc.pl.umap(xdata,color=['antipode_cluster'],use_raw=False,legend_loc=None,palette=sc.pl.palettes.godsnot_102)\n",
    "\"\"\"\n",
    "# xdata.X=xdata.layers['UMIs']\n",
    "# sc.pp.normalize_per_cell(xdata)\n",
    "# sc.pp.log1p(xdata)\n",
    "# sc.pp.scale(xdata,max_value=10)\n",
    "#sc.pl.umap(xdata,color=['GBX2','EOMES','SIX3','OTX2','FOXG1','RBFOX3','TH','PDGFRA','AQP4','FOXJ1','AIF1','TTR','MOG','COL1A2','CD34','COL4A1','NPY','NKX2-1','FOXP2','SATB2','RORB','FEZF2','EMX1'],use_raw=False,cmap='Purples')\n",
    "#sc.pl.umap(xdata,color=['DLX2','PROX1','SCGN','TSHZ1','MEIS2','NKX2-1','LHX6','CRABP1','TSHZ1','FOXG1','PDGFRA','AIF1','AQP4','EDNRB','FOXJ1','CD34','MKI67'],cmap='Purples',use_raw=False)\n",
    "#sc.pl.umap(xdata,color=['RPL7','RPS17','RPL13A','MEF2C'],cmap='Purples',use_raw=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a8088c-0ca6-4224-b9c3-5d53125d74bb",
   "metadata": {},
   "source": [
    "# Training Phase 3: Refining the final tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b8a556-5e56-4aa0-9c3d-f7514c48e4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "antipode_model.train_phase_3(max_steps=max_steps,print_every=10000,num_particles=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82908e0c-eb8f-421f-bca7-fe6b29192a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(antipode_model.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7304689d-821f-4521-b515-414283bef218",
   "metadata": {},
   "outputs": [],
   "source": [
    "antipode_model.store_outputs(device=device,prefix='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b70a9-0c3e-4615-ad65-9dd3b6c9b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gmm_heatmaps(antipode_model)\n",
    "plot_tree_edge_weights(antipode_model)\n",
    "plot_d_hists(antipode_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7e5701-b786-4267-bb7b-db00eada2a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MDE_KEY = \"X_antipode_MDE\"\n",
    "adata.obsm[MDE_KEY] = scvi.model.utils.mde(adata.obsm['X_antipode'])\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[\"antipode_cluster\",\"kmeans\"],legend_fontsize=6,legend_fontweight='normal',\n",
    "    legend_loc='on data',palette=sc.pl.palettes.godsnot_102\n",
    ")\n",
    "\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[\"species\"],palette=sc.pl.palettes.godsnot_102\n",
    ")\n",
    "\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[\"batch_name\"],palette=sc.pl.palettes.godsnot_102\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3acebd-8220-45de-9581-8ccead8a509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[x for x in antipode_model.adata_manager.adata.obs.columns if 'level' in x]+[\"antipode_cluster\"],\n",
    "    palette=sc.pl.palettes.godsnot_102,legend_fontsize=6,\n",
    "    legend_loc='on data'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf8b601-4c7a-42ff-aa31-509cb4585311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.pl.umap(adata,color=[x for x in adata.obs.columns if 'level' in x]+['antipode_cluster','kmeans'],use_raw=False,legend_loc=None,palette=sc.pl.palettes.godsnot_102)\n",
    "\n",
    "random_choice=np.random.choice(adata.obs.index,size=100000,replace=False)\n",
    "random_choice=np.where(adata.obs.index.isin(random_choice))[0]\n",
    "xdata=adata[random_choice,:]\n",
    "xdata=xdata.to_memory().copy()\n",
    "\"\"\"\n",
    "sc.pp.neighbors(xdata,n_neighbors=20, use_rep=\"X_antipode\")\n",
    "sc.tl.umap(xdata)\n",
    "\n",
    "sc.pl.umap(xdata,color=['species'],use_raw=False,palette=sc.pl.palettes.godsnot_102)\n",
    "sc.pl.umap(xdata,color=['psi'],use_raw=False,cmap='coolwarm')\n",
    "sc.pl.umap(xdata,color=['antipode_cluster'],use_raw=False,legend_loc=None,palette=sc.pl.palettes.godsnot_102)\n",
    "\"\"\"\n",
    "# xdata.X=xdata.layers['UMIs'].todense()\n",
    "# sc.pp.normalize_per_cell(xdata)\n",
    "# sc.pp.log1p(xdata)\n",
    "# sc.pp.scale(xdata,max_value=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4392c8f5-2329-467a-9f8d-79f490e85428",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata.X=xdata.raw.X[:,adata.raw.var.index.isin(adata.var.index)].todense()\n",
    "sc.pp.normalize_per_cell(xdata)\n",
    "sc.pp.log1p(xdata)\n",
    "\n",
    "gene_list=['RBFOX3','PDGFRA','AQP4','FOXJ1','AIF1','MOG','COL1A2','CD34','COL4A1','NPY','FOXP2','SATB2','RORB','DLX2','PROX1','SCGN','TSHZ1','SLC17A7','TLE4',\n",
    "           'MEIS2','NKX2-1','LHX6','CRABP1','TSHZ1','FOXG1','PDGFRA','AIF1','AQP4','EDNRB','FOXJ1','CD34','MKI67','RPL7','RPS17','RPL13A','MEF2C']\n",
    "gene_list=[x for x in gene_list if x in xdata.var.index]\n",
    "sc.pl.embedding(\n",
    "    xdata,\n",
    "    basis=MDE_KEY,\n",
    "    color=gene_list,cmap='Purples',\n",
    "    palette=sc.pl.palettes.godsnot_102,legend_fontsize=6,\n",
    "    legend_loc='on data',use_raw=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1360aa7-5a6e-49ec-9eaf-1ff9b834fc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.write_h5ad('/home/matthew.schmitz/Matthew/1.9.1_dev_run.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb85cef-b77b-4da5-a611-8c22e055e556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6537fa-ea33-4389-95ff-0a34ec634704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e9528f-96f6-48e6-be51-c8f14ea3173a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyro",
   "language": "python",
   "name": "pyro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
